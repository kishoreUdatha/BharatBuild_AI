"""
ML Projects API - Endpoints for ML project generation with templates + AI customization

Endpoints:
- GET /ml/models - List available ML models
- GET /ml/models/{model_type} - Get model details and config options
- POST /ml/generate - Generate ML project from template
- POST /ml/customize - Generate ML project with AI customization
- GET /ml/template/{model_type} - Get raw template files
"""

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from typing import Optional
import json

from app.core.database import get_db
from app.core.config import settings
from app.core.logging_config import logger
from app.models.user import User
from app.models.project import Project, ProjectStatus, ProjectMode
from app.models.project_file import ProjectFile
from app.modules.auth.dependencies import get_current_user
from app.services.ml_templates_service import ml_templates_service, MLModel, MLCategory, MLFramework
from app.schemas.ml_project import (
    MLProjectConfig,
    MLProjectCreateRequest,
    MLCustomizationRequest,
    MLModelInfo,
    MLModelsListResponse,
    MLTemplateFile,
    MLTemplateResponse,
    MLProjectResponse,
    MLConfigOptions
)
from app.schemas.dataset import (
    MLGenerateWithDatasetRequest,
    MLGenerateWithDatasetResponse,
    MLGenerateWithImageDatasetRequest,
    MLGenerateWithImageDatasetResponse,
    is_tabular_model,
    is_vision_model,
    TABULAR_ML_MODELS,
    VISION_ML_MODELS,
    DatasetType as SchemaDatasetType,
)
from app.models.dataset import Dataset, DatasetStatus, DatasetType

router = APIRouter(prefix="/ml", tags=["ML Projects"])


# ==================== Helper Functions ====================

def get_language_from_filename(filename: str) -> str:
    """Detect language from filename"""
    ext_map = {
        ".py": "python",
        ".yaml": "yaml",
        ".yml": "yaml",
        ".json": "json",
        ".md": "markdown",
        ".txt": "text",
        ".sh": "bash",
        ".dockerfile": "dockerfile",
    }
    for ext, lang in ext_map.items():
        if filename.lower().endswith(ext):
            return lang
    return "text"


def apply_config_to_template(content: str, config: dict) -> str:
    """Apply configuration values to template placeholders"""
    for key, value in config.items():
        placeholder = f"{{{{{key}}}}}"
        content = content.replace(placeholder, str(value))
    return content


def generate_data_loader_code(
    dataset_name: str,
    filename: str,
    target_column: str,
    feature_columns: list,
    columns_info: list
) -> str:
    """
    Generate Python data loader code for a dataset.

    Returns ready-to-use code for loading and preprocessing the CSV data.
    """
    # Determine which columns are categorical
    categorical_cols = []
    numeric_cols = []
    for col in columns_info:
        col_name = col.get("name")
        if col_name in feature_columns:
            if col.get("is_categorical"):
                categorical_cols.append(col_name)
            elif col.get("is_numeric"):
                numeric_cols.append(col_name)

    # Generate the data loader code
    code = f'''"""
Data Loader for {dataset_name}
Auto-generated by BharatBuild ML Project Generator

This module provides functions to load and preprocess the dataset for ML training.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from typing import Tuple, Optional

# ==================== Dataset Configuration ====================

DATASET_PATH = "data/{filename}"
TARGET_COLUMN = "{target_column}"
FEATURE_COLUMNS = {feature_columns}

# Column categorization
NUMERIC_COLUMNS = {numeric_cols}
CATEGORICAL_COLUMNS = {categorical_cols}


# ==================== Data Loading Functions ====================

def load_raw_data(path: str = DATASET_PATH) -> pd.DataFrame:
    """
    Load the raw CSV data.

    Args:
        path: Path to the CSV file

    Returns:
        DataFrame with raw data
    """
    df = pd.read_csv(path)
    print(f"Loaded {{len(df)}} rows with {{len(df.columns)}} columns")
    return df


def load_data(path: str = DATASET_PATH) -> Tuple[pd.DataFrame, pd.Series]:
    """
    Load and split data into features (X) and target (y).

    Args:
        path: Path to the CSV file

    Returns:
        Tuple of (X features DataFrame, y target Series)
    """
    df = load_raw_data(path)

    # Extract features and target
    X = df[FEATURE_COLUMNS].copy()
    y = df[TARGET_COLUMN].copy()

    return X, y


def get_preprocessor() -> ColumnTransformer:
    """
    Create a preprocessing pipeline for the features.

    Returns:
        ColumnTransformer that handles numeric scaling and categorical encoding
    """
    transformers = []

    # Numeric columns: StandardScaler
    if NUMERIC_COLUMNS:
        transformers.append(
            ("numeric", StandardScaler(), NUMERIC_COLUMNS)
        )

    # Categorical columns: OneHotEncoder
    if CATEGORICAL_COLUMNS:
        transformers.append(
            ("categorical", OneHotEncoder(handle_unknown="ignore", sparse_output=False), CATEGORICAL_COLUMNS)
        )

    return ColumnTransformer(
        transformers=transformers,
        remainder="passthrough"  # Keep any other columns unchanged
    )


def preprocess_data(
    X: pd.DataFrame,
    y: pd.Series,
    preprocessor: Optional[ColumnTransformer] = None,
    fit: bool = True
) -> Tuple[np.ndarray, np.ndarray, ColumnTransformer]:
    """
    Preprocess features and encode target if needed.

    Args:
        X: Feature DataFrame
        y: Target Series
        preprocessor: Optional pre-fitted preprocessor
        fit: Whether to fit the preprocessor (True for training, False for inference)

    Returns:
        Tuple of (X_processed, y_processed, preprocessor)
    """
    if preprocessor is None:
        preprocessor = get_preprocessor()

    # Transform features
    if fit:
        X_processed = preprocessor.fit_transform(X)
    else:
        X_processed = preprocessor.transform(X)

    # Encode target if categorical
    y_processed = y.values
    if y.dtype == "object" or y.dtype.name == "category":
        le = LabelEncoder()
        y_processed = le.fit_transform(y)
        print(f"Target classes: {{le.classes_}}")

    return X_processed, y_processed, preprocessor


def get_train_test_split(
    test_size: float = 0.2,
    random_state: int = 42,
    preprocess: bool = True
) -> dict:
    """
    Load data and split into train/test sets.

    Args:
        test_size: Proportion of data for testing (default 0.2)
        random_state: Random seed for reproducibility
        preprocess: Whether to preprocess the data

    Returns:
        Dictionary with X_train, X_test, y_train, y_test, and preprocessor
    """
    X, y = load_data()

    # Split first (before preprocessing to avoid data leakage)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )

    preprocessor = None
    if preprocess:
        # Fit on training data, transform both
        X_train, y_train, preprocessor = preprocess_data(X_train, y_train, fit=True)
        X_test, y_test, _ = preprocess_data(X_test, y_test, preprocessor=preprocessor, fit=False)

    print(f"Train set: {{len(X_train)}} samples")
    print(f"Test set: {{len(X_test)}} samples")

    return {{
        "X_train": X_train,
        "X_test": X_test,
        "y_train": y_train,
        "y_test": y_test,
        "preprocessor": preprocessor,
    }}


def get_data_info() -> dict:
    """
    Get information about the dataset configuration.

    Returns:
        Dictionary with dataset metadata
    """
    return {{
        "dataset_path": DATASET_PATH,
        "target_column": TARGET_COLUMN,
        "feature_columns": FEATURE_COLUMNS,
        "numeric_columns": NUMERIC_COLUMNS,
        "categorical_columns": CATEGORICAL_COLUMNS,
        "num_features": len(FEATURE_COLUMNS),
    }}


# ==================== Usage Example ====================

if __name__ == "__main__":
    # Load and preview data
    df = load_raw_data()
    print("\\nDataset preview:")
    print(df.head())
    print("\\nDataset info:")
    print(df.info())

    # Get train/test split
    data = get_train_test_split(test_size=0.2)
    print(f"\\nX_train shape: {{data['X_train'].shape}}")
    print(f"X_test shape: {{data['X_test'].shape}}")
'''

    return code


def generate_image_loader_code(
    dataset_name: str,
    zip_filename: str,
    class_names: list,
    input_size: int = 224,
    augmentation: bool = True,
    pretrained: bool = True
) -> str:
    """
    Generate Python image loader code for an image dataset.

    Returns ready-to-use code for loading and preprocessing images.
    """
    class_names_str = str(class_names)

    code = f'''"""
Image Data Loader for {dataset_name}
Auto-generated by BharatBuild ML Project Generator

This module provides functions to load and preprocess the image dataset for training.
"""

import os
import torch
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms, datasets
from PIL import Image
import zipfile
from pathlib import Path
from typing import Tuple, Optional, Dict, Any
import shutil

# ==================== Dataset Configuration ====================

DATASET_ZIP = "data/{zip_filename}"
DATASET_DIR = "data/extracted_images"
INPUT_SIZE = {input_size}
CLASS_NAMES = {class_names_str}
NUM_CLASSES = {len(class_names)}

# ImageNet normalization (for pretrained models)
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]


# ==================== Transforms ====================

def get_train_transforms(input_size: int = INPUT_SIZE, augmentation: bool = {str(augmentation).lower()}):
    """Get training transforms with optional augmentation."""
    transform_list = [
        transforms.Resize((input_size, input_size)),
    ]

    if augmentation:
        transform_list.extend([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        ])

    transform_list.extend([
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])

    return transforms.Compose(transform_list)


def get_val_transforms(input_size: int = INPUT_SIZE):
    """Get validation/test transforms (no augmentation)."""
    return transforms.Compose([
        transforms.Resize((input_size, input_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),
    ])


# ==================== Data Loading ====================

def extract_dataset(zip_path: str = DATASET_ZIP, extract_dir: str = DATASET_DIR) -> str:
    """
    Extract the dataset ZIP file.

    Args:
        zip_path: Path to the ZIP file
        extract_dir: Directory to extract to

    Returns:
        Path to extracted dataset root
    """
    if os.path.exists(extract_dir):
        print(f"Dataset already extracted at {{extract_dir}}")
        return extract_dir

    print(f"Extracting {{zip_path}} to {{extract_dir}}...")
    os.makedirs(extract_dir, exist_ok=True)

    with zipfile.ZipFile(zip_path, 'r') as zf:
        zf.extractall(extract_dir)

    print(f"Extracted to {{extract_dir}}")
    return extract_dir


def find_data_root(extract_dir: str = DATASET_DIR) -> str:
    """
    Find the actual data root (handles nested folders).

    Returns the directory containing class folders.
    """
    # Check if extract_dir directly contains class folders
    subdirs = [d for d in os.listdir(extract_dir) if os.path.isdir(os.path.join(extract_dir, d))]

    # If subdirs are train/test/val, use train as root
    if 'train' in subdirs:
        return os.path.join(extract_dir, 'train')

    # If there's only one subfolder, go deeper
    if len(subdirs) == 1 and subdirs[0] not in CLASS_NAMES:
        return find_data_root(os.path.join(extract_dir, subdirs[0]))

    return extract_dir


def get_datasets(
    data_dir: Optional[str] = None,
    train_transform: Optional[transforms.Compose] = None,
    val_transform: Optional[transforms.Compose] = None,
    val_split: float = 0.2
) -> Tuple[Dataset, Dataset]:
    """
    Load and split the image dataset.

    Args:
        data_dir: Path to extracted images (or None to auto-extract)
        train_transform: Transforms for training data
        val_transform: Transforms for validation data
        val_split: Proportion for validation set

    Returns:
        Tuple of (train_dataset, val_dataset)
    """
    if data_dir is None:
        extract_dataset()
        data_dir = find_data_root()

    if train_transform is None:
        train_transform = get_train_transforms()
    if val_transform is None:
        val_transform = get_val_transforms()

    # Load full dataset with train transforms first
    full_dataset = datasets.ImageFolder(data_dir, transform=train_transform)

    # Split into train and val
    total_size = len(full_dataset)
    val_size = int(total_size * val_split)
    train_size = total_size - val_size

    train_dataset, val_dataset = random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )

    # Apply val transforms to validation set
    class TransformSubset(Dataset):
        def __init__(self, subset, transform):
            self.subset = subset
            self.transform = transform

        def __getitem__(self, idx):
            img, label = self.subset.dataset.samples[self.subset.indices[idx]]
            img = Image.open(img).convert('RGB')
            if self.transform:
                img = self.transform(img)
            return img, label

        def __len__(self):
            return len(self.subset)

    val_dataset = TransformSubset(val_dataset, val_transform)

    print(f"Train set: {{train_size}} images")
    print(f"Val set: {{val_size}} images")
    print(f"Classes: {{full_dataset.classes}}")

    return train_dataset, val_dataset


def get_data_loaders(
    batch_size: int = 32,
    num_workers: int = 4,
    val_split: float = 0.2
) -> Dict[str, DataLoader]:
    """
    Get train and validation data loaders.

    Args:
        batch_size: Batch size for training
        num_workers: Number of data loading workers
        val_split: Validation split proportion

    Returns:
        Dictionary with 'train' and 'val' DataLoaders
    """
    train_dataset, val_dataset = get_datasets(val_split=val_split)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return {{
        'train': train_loader,
        'val': val_loader
    }}


def get_class_weights(data_dir: Optional[str] = None) -> torch.Tensor:
    """
    Calculate class weights for imbalanced datasets.

    Returns:
        Tensor of class weights
    """
    if data_dir is None:
        extract_dataset()
        data_dir = find_data_root()

    dataset = datasets.ImageFolder(data_dir)
    class_counts = torch.zeros(NUM_CLASSES)

    for _, label in dataset.samples:
        class_counts[label] += 1

    # Inverse frequency weighting
    weights = 1.0 / class_counts
    weights = weights / weights.sum() * NUM_CLASSES

    return weights


def get_data_info() -> Dict[str, Any]:
    """Get information about the dataset configuration."""
    return {{
        "dataset_zip": DATASET_ZIP,
        "input_size": INPUT_SIZE,
        "class_names": CLASS_NAMES,
        "num_classes": NUM_CLASSES,
        "imagenet_mean": IMAGENET_MEAN,
        "imagenet_std": IMAGENET_STD,
    }}


# ==================== Usage Example ====================

if __name__ == "__main__":
    # Extract and load data
    extract_dataset()
    data_root = find_data_root()
    print(f"\\nData root: {{data_root}}")

    # Get data loaders
    loaders = get_data_loaders(batch_size=32)
    print(f"\\nTrain batches: {{len(loaders['train'])}}")
    print(f"Val batches: {{len(loaders['val'])}}")

    # Test a batch
    images, labels = next(iter(loaders['train']))
    print(f"\\nBatch shape: {{images.shape}}")
    print(f"Labels: {{labels[:8]}}")
'''

    return code


# ==================== Endpoints ====================

@router.get("/models", response_model=MLModelsListResponse)
async def list_ml_models():
    """
    List all available ML models/templates.

    Returns models organized by category with descriptions and use cases.
    """
    models = ml_templates_service.get_available_models()

    return MLModelsListResponse(
        models=[MLModelInfo(**m) for m in models],
        total=len(models)
    )


@router.get("/models/{model_type}", response_model=MLConfigOptions)
async def get_model_details(model_type: str):
    """
    Get configuration options for a specific model type.

    Returns available configuration parameters and their defaults.
    """
    try:
        ml_model = MLModel(model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Model type '{model_type}' not found"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Template not found for model: {model_type}"
        )

    # Define configurable options based on model type
    options = {
        "num_classes": {"type": "integer", "min": 2, "description": "Number of output classes"},
        "batch_size": {"type": "integer", "min": 1, "max": 512, "description": "Training batch size"},
        "epochs": {"type": "integer", "min": 1, "max": 1000, "description": "Number of training epochs"},
        "learning_rate": {"type": "float", "min": 0.00001, "max": 1.0, "description": "Learning rate"},
    }

    # Add model-specific options
    if template.category in [MLCategory.COMPUTER_VISION]:
        options["input_size"] = {"type": "integer", "min": 32, "max": 1024, "description": "Input image size"}

    if template.category in [MLCategory.NLP]:
        options["max_length"] = {"type": "integer", "min": 32, "max": 2048, "description": "Max sequence length"}
        options["hidden_dim"] = {"type": "integer", "min": 32, "max": 1024, "description": "Hidden layer size"}

    return MLConfigOptions(
        model_type=model_type,
        options=options,
        defaults=template.config,
        description=template.description
    )


@router.get("/template/{model_type}", response_model=MLTemplateResponse)
async def get_template(
    model_type: str,
    project_name: str = "my_ml_project"
):
    """
    Get raw template files for a model type.

    Returns all template files without saving to database.
    Useful for previewing before creating a project.
    """
    try:
        ml_model = MLModel(model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Model type '{model_type}' not found"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Template not found for model: {model_type}"
        )

    # Generate files with default config
    files = ml_templates_service.generate_project(ml_model, project_name, template.config)

    return MLTemplateResponse(
        model_type=model_type,
        project_name=project_name,
        files=[
            MLTemplateFile(
                path=path,
                content=content,
                language=get_language_from_filename(path)
            )
            for path, content in files.items()
        ],
        requirements=template.requirements,
        config=template.config
    )


@router.post("/generate", response_model=MLProjectResponse)
async def generate_ml_project(
    request: MLProjectCreateRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Generate a new ML project from template.

    Creates a project with all necessary files for the selected model type.
    Applies custom configuration if provided.
    """
    config = request.config

    try:
        ml_model = MLModel(config.model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid model type: {config.model_type}"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Template not found for model: {config.model_type}"
        )

    # Merge custom config with defaults
    final_config = {**template.config}
    if config.num_classes:
        final_config["num_classes"] = config.num_classes
    if config.input_size:
        final_config["input_size"] = config.input_size
    if config.max_length:
        final_config["max_length"] = config.max_length
    if config.hidden_dim:
        final_config["hidden_dim"] = config.hidden_dim
    if config.num_layers:
        final_config["num_layers"] = config.num_layers
    if config.learning_rate:
        final_config["learning_rate"] = config.learning_rate
    if config.batch_size:
        final_config["batch_size"] = config.batch_size
    if config.epochs:
        final_config["epochs"] = config.epochs

    # Generate files
    files = ml_templates_service.generate_project(ml_model, config.project_name, final_config)

    # Create project in database
    project = Project(
        user_id=str(current_user.id),
        workspace_id=request.workspace_id,
        title=config.project_name,
        description=f"ML Project: {template.name} - {template.description}",
        mode=ProjectMode.DEVELOPER,
        status=ProjectStatus.COMPLETED,
        config={
            "ml_model": config.model_type,
            "ml_category": template.category.value,
            "ml_framework": template.framework.value,
            "ml_config": final_config
        },
        framework=template.framework.value,
        technology=f"Python, {template.framework.value.title()}, ML"
    )
    db.add(project)
    await db.flush()

    # Create project files
    for file_path, content in files.items():
        project_file = ProjectFile(
            project_id=project.id,
            path=file_path,
            name=file_path.split("/")[-1],
            content_inline=content,
            is_inline=True,
            language=get_language_from_filename(file_path),
            size_bytes=len(content.encode('utf-8'))
        )
        db.add(project_file)

    await db.commit()

    logger.info(f"Created ML project {project.id} ({config.model_type}) for user {current_user.id}")

    return MLProjectResponse(
        project_id=str(project.id),
        project_name=config.project_name,
        model_type=config.model_type,
        category=template.category.value,
        framework=template.framework.value,
        files_created=len(files),
        message=f"ML project created successfully with {len(files)} files"
    )


@router.post("/customize", response_model=MLProjectResponse)
async def customize_ml_project(
    request: MLCustomizationRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Generate ML project with AI customization.

    Starts from a template (if base_template=True) and uses AI to customize
    based on the user's specific requirements.
    """
    try:
        ml_model = MLModel(request.model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid model type: {request.model_type}"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Template not found for model: {request.model_type}"
        )

    # Get base template files
    base_config = {**template.config, **(request.config or {})}
    base_files = ml_templates_service.generate_project(ml_model, request.project_name, base_config)

    # Use AI to customize (integrate with existing orchestrator/bolt system)
    # For now, we'll create the project with base template and store the customization prompt
    # The orchestrator can pick this up and enhance the files

    # Create project
    project = Project(
        user_id=str(current_user.id),
        title=request.project_name,
        description=f"ML Project: {template.name} - Customized with AI\n\nRequirements: {request.prompt}",
        mode=ProjectMode.DEVELOPER,
        status=ProjectStatus.PARTIAL_COMPLETED,  # Will be enhanced by AI
        config={
            "ml_model": request.model_type,
            "ml_category": template.category.value,
            "ml_framework": template.framework.value,
            "ml_config": base_config,
            "customization_prompt": request.prompt,
            "ai_customization_pending": True
        },
        framework=template.framework.value,
        technology=f"Python, {template.framework.value.title()}, ML"
    )
    db.add(project)
    await db.flush()

    # Create base files
    for file_path, content in base_files.items():
        project_file = ProjectFile(
            project_id=project.id,
            path=file_path,
            name=file_path.split("/")[-1],
            content_inline=content,
            is_inline=True,
            language=get_language_from_filename(file_path),
            size_bytes=len(content.encode('utf-8'))
        )
        db.add(project_file)

    await db.commit()

    logger.info(f"Created customized ML project {project.id} ({request.model_type}) for user {current_user.id}")

    return MLProjectResponse(
        project_id=str(project.id),
        project_name=request.project_name,
        model_type=request.model_type,
        category=template.category.value,
        framework=template.framework.value,
        files_created=len(base_files),
        message=f"ML project created with base template. AI customization will be applied based on your requirements."
    )


@router.get("/categories")
async def list_categories():
    """List all ML categories"""
    return {
        "categories": [
            {"id": c.value, "name": c.value.replace("_", " ").title()}
            for c in MLCategory
        ]
    }


@router.get("/frameworks")
async def list_frameworks():
    """List all supported ML frameworks"""
    return {
        "frameworks": [
            {"id": f.value, "name": f.value.title()}
            for f in MLFramework
        ]
    }


@router.get("/tabular-models")
async def list_tabular_models():
    """List ML models that support CSV datasets"""
    return {
        "models": TABULAR_ML_MODELS,
        "description": "These models support CSV dataset upload for training data"
    }


@router.post("/generate-with-dataset", response_model=MLGenerateWithDatasetResponse)
async def generate_with_dataset(
    request: MLGenerateWithDatasetRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Generate ML project with an uploaded CSV dataset.

    This endpoint:
    1. Validates the model type supports tabular data
    2. Retrieves the dataset and validates it's ready
    3. Generates the ML template files
    4. Creates a custom data_loader.py for the dataset
    5. Creates the project with all files

    The generated project includes:
    - Model training code
    - data/data_loader.py - Custom loader for the dataset
    - Configuration files
    - README with instructions
    """
    # Validate model type supports CSV
    if not is_tabular_model(request.model_type):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Model type '{request.model_type}' does not support CSV datasets. Supported models: {TABULAR_ML_MODELS}"
        )

    # Get dataset
    result = await db.execute(
        select(Dataset).where(
            Dataset.id == request.dataset_id,
            Dataset.user_id == str(current_user.id)
        )
    )
    dataset = result.scalar_one_or_none()

    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Dataset not found"
        )

    # Use request target column or fall back to dataset's configured target
    target_column = request.target_column or dataset.target_column
    if not target_column:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Target column must be specified (either in request or pre-configured in dataset)"
        )

    # Validate target column exists in dataset
    column_names = [col.get("name") for col in dataset.columns] if dataset.columns else []
    if target_column not in column_names:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Target column '{target_column}' not found in dataset"
        )

    # Determine feature columns
    if request.feature_columns:
        feature_columns = request.feature_columns
        # Validate all feature columns exist
        invalid_cols = [c for c in feature_columns if c not in column_names]
        if invalid_cols:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Feature columns not found: {invalid_cols}"
            )
    else:
        # Default to all columns except target
        feature_columns = dataset.feature_columns or [c for c in column_names if c != target_column]

    # Get ML template
    try:
        ml_model = MLModel(request.model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid model type: {request.model_type}"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Template not found for model: {request.model_type}"
        )

    # Build config
    final_config = {**template.config}
    if request.num_classes:
        final_config["num_classes"] = request.num_classes
    if request.batch_size:
        final_config["batch_size"] = request.batch_size
    if request.epochs:
        final_config["epochs"] = request.epochs
    if request.learning_rate:
        final_config["learning_rate"] = request.learning_rate

    # Generate template files
    files = ml_templates_service.generate_project(ml_model, request.project_name, final_config)

    # Generate custom data loader
    data_loader_code = generate_data_loader_code(
        dataset_name=dataset.name,
        filename=dataset.original_filename,
        target_column=target_column,
        feature_columns=feature_columns,
        columns_info=dataset.columns or []
    )
    files["data/data_loader.py"] = data_loader_code

    # Add dataset info to README if exists
    if "README.md" in files:
        dataset_section = f"""

## Dataset

This project is configured to use the **{dataset.name}** dataset.

- **File**: `data/{dataset.original_filename}`
- **Rows**: {dataset.row_count}
- **Target Column**: `{target_column}`
- **Feature Columns**: {len(feature_columns)} columns

### Loading Data

```python
from data.data_loader import get_train_test_split

# Get preprocessed train/test data
data = get_train_test_split(test_size=0.2)
X_train, X_test = data['X_train'], data['X_test']
y_train, y_test = data['y_train'], data['y_test']
```
"""
        files["README.md"] = files["README.md"] + dataset_section

    # Create project in database
    project = Project(
        user_id=str(current_user.id),
        workspace_id=request.workspace_id,
        title=request.project_name,
        description=f"ML Project: {template.name} with dataset '{dataset.name}'",
        mode=ProjectMode.DEVELOPER,
        status=ProjectStatus.COMPLETED,
        config={
            "ml_model": request.model_type,
            "ml_category": template.category.value,
            "ml_framework": template.framework.value,
            "ml_config": final_config,
            "dataset_id": str(dataset.id),
            "target_column": target_column,
            "feature_columns": feature_columns,
        },
        framework=template.framework.value,
        technology=f"Python, {template.framework.value.title()}, ML, CSV Data"
    )
    db.add(project)
    await db.flush()

    # Associate dataset with project
    dataset.project_id = str(project.id)
    dataset.target_column = target_column
    dataset.feature_columns = feature_columns
    dataset.status = DatasetStatus.READY

    # Create project files
    for file_path, content in files.items():
        project_file = ProjectFile(
            project_id=project.id,
            path=file_path,
            name=file_path.split("/")[-1],
            content_inline=content,
            is_inline=True,
            language=get_language_from_filename(file_path),
            size_bytes=len(content.encode('utf-8'))
        )
        db.add(project_file)

    await db.commit()

    logger.info(f"Created ML project {project.id} with dataset {dataset.id} for user {current_user.id}")

    return MLGenerateWithDatasetResponse(
        project_id=str(project.id),
        project_name=request.project_name,
        model_type=request.model_type,
        dataset_id=str(dataset.id),
        files_created=len(files),
        data_loader_path="data/data_loader.py",
        message=f"ML project created with {len(files)} files including custom data loader for '{dataset.name}'"
    )


@router.get("/vision-models")
async def list_vision_models():
    """List ML models that support image datasets"""
    return {
        "models": VISION_ML_MODELS,
        "description": "These models support image dataset upload (ZIP with folders)"
    }


@router.post("/generate-with-image-dataset", response_model=MLGenerateWithImageDatasetResponse)
async def generate_with_image_dataset(
    request: MLGenerateWithImageDatasetRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Generate ML project with an uploaded image dataset.

    This endpoint:
    1. Validates the model type supports images
    2. Retrieves the image dataset and validates it's ready
    3. Generates the ML template files
    4. Creates a custom image_loader.py for the dataset
    5. Creates the project with all files

    The generated project includes:
    - Model training code (CNN, ResNet, etc.)
    - data/image_loader.py - Custom loader for the image dataset
    - Configuration files
    - README with instructions
    """
    # Validate model type supports images
    if not is_vision_model(request.model_type):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Model type '{request.model_type}' does not support image datasets. Supported models: {VISION_ML_MODELS}"
        )

    # Get dataset
    result = await db.execute(
        select(Dataset).where(
            Dataset.id == request.dataset_id,
            Dataset.user_id == str(current_user.id),
            Dataset.dataset_type == DatasetType.IMAGE
        )
    )
    dataset = result.scalar_one_or_none()

    if not dataset:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Image dataset not found"
        )

    # Validate dataset has classes
    if not dataset.classes or not dataset.num_classes:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Dataset has no detected classes"
        )

    if dataset.num_classes < 2:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Dataset must have at least 2 classes"
        )

    # Get class names
    class_names = [cls.get("name") for cls in dataset.classes] if dataset.classes else []

    # Determine configuration
    num_classes = request.num_classes or dataset.num_classes
    input_size = request.input_size or dataset.recommended_input_size or 224

    # Get ML template
    try:
        ml_model = MLModel(request.model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid model type: {request.model_type}"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Template not found for model: {request.model_type}"
        )

    # Build config
    final_config = {**template.config}
    final_config["num_classes"] = num_classes
    final_config["input_size"] = input_size
    if request.batch_size:
        final_config["batch_size"] = request.batch_size
    if request.epochs:
        final_config["epochs"] = request.epochs
    if request.learning_rate:
        final_config["learning_rate"] = request.learning_rate

    # Generate template files
    files = ml_templates_service.generate_project(ml_model, request.project_name, final_config)

    # Generate custom image loader
    image_loader_code = generate_image_loader_code(
        dataset_name=dataset.name,
        zip_filename=dataset.original_filename,
        class_names=class_names,
        input_size=input_size,
        augmentation=request.augmentation if request.augmentation is not None else True,
        pretrained=request.pretrained if request.pretrained is not None else True
    )
    files["data/image_loader.py"] = image_loader_code

    # Add dataset info to README if exists
    if "README.md" in files:
        dataset_section = f"""

## Dataset

This project is configured to use the **{dataset.name}** image dataset.

- **File**: `data/{dataset.original_filename}`
- **Classes**: {num_classes} ({', '.join(class_names[:5])}{'...' if len(class_names) > 5 else ''})
- **Total Images**: {dataset.total_images}
- **Input Size**: {input_size}x{input_size}

### Loading Data

```python
from data.image_loader import get_data_loaders, extract_dataset

# Extract the dataset (first time only)
extract_dataset()

# Get train and validation data loaders
loaders = get_data_loaders(batch_size=32)
train_loader = loaders['train']
val_loader = loaders['val']

# Training loop
for images, labels in train_loader:
    # images: (batch_size, 3, {input_size}, {input_size})
    # labels: (batch_size,) with values 0 to {num_classes-1}
    pass
```

### Class Names
```python
CLASS_NAMES = {class_names}
```
"""
        files["README.md"] = files["README.md"] + dataset_section

    # Create project in database
    project = Project(
        user_id=str(current_user.id),
        workspace_id=request.workspace_id,
        title=request.project_name,
        description=f"ML Project: {template.name} with image dataset '{dataset.name}' ({num_classes} classes)",
        mode=ProjectMode.DEVELOPER,
        status=ProjectStatus.COMPLETED,
        config={
            "ml_model": request.model_type,
            "ml_category": template.category.value,
            "ml_framework": template.framework.value,
            "ml_config": final_config,
            "dataset_id": str(dataset.id),
            "dataset_type": "image",
            "num_classes": num_classes,
            "class_names": class_names,
            "input_size": input_size,
            "pretrained": request.pretrained,
            "augmentation": request.augmentation,
        },
        framework=template.framework.value,
        technology=f"Python, {template.framework.value.title()}, Computer Vision, PyTorch"
    )
    db.add(project)
    await db.flush()

    # Associate dataset with project
    dataset.project_id = str(project.id)
    dataset.status = DatasetStatus.READY

    # Create project files
    for file_path, content in files.items():
        project_file = ProjectFile(
            project_id=project.id,
            path=file_path,
            name=file_path.split("/")[-1],
            content_inline=content,
            is_inline=True,
            language=get_language_from_filename(file_path),
            size_bytes=len(content.encode('utf-8'))
        )
        db.add(project_file)

    await db.commit()

    logger.info(f"Created vision ML project {project.id} with image dataset {dataset.id} ({num_classes} classes) for user {current_user.id}")

    return MLGenerateWithImageDatasetResponse(
        project_id=str(project.id),
        project_name=request.project_name,
        model_type=request.model_type,
        dataset_id=str(dataset.id),
        num_classes=num_classes,
        input_size=input_size,
        files_created=len(files),
        data_loader_path="data/image_loader.py",
        message=f"Vision ML project created with {len(files)} files including custom image loader for '{dataset.name}' ({num_classes} classes)"
    )


# ==================== Prompt-Based Generation ====================

# Import the new schemas
from app.schemas.ml_project import (
    PromptAnalysisResult,
    PromptMLGenerateRequest,
    PromptMLGenerateResponse,
    PromptAnalyzeRequest,
    PromptAnalyzeResponse
)
import re


def analyze_ml_prompt(prompt: str) -> PromptAnalysisResult:
    """
    Analyze natural language prompt to extract ML project configuration.

    This function uses pattern matching and keyword detection to understand
    what kind of ML project the user wants to create.
    """
    prompt_lower = prompt.lower()

    # ==================== Detect Model Type ====================

    model_patterns = {
        # Vision models
        "cnn": ["cnn", "convolutional", "image classification", "classify images"],
        "resnet": ["resnet", "residual network", "res-net"],
        "vgg": ["vgg", "vgg16", "vgg19"],
        "efficientnet": ["efficientnet", "efficient net"],
        "yolo": ["yolo", "object detection", "detect objects", "bounding box"],
        "unet": ["unet", "u-net", "segmentation", "image segmentation"],
        "mobilenet": ["mobilenet", "mobile net", "lightweight cnn"],

        # NLP models
        "bert": ["bert", "text classification", "sentiment analysis", "nlp classification"],
        "lstm": ["lstm", "long short term", "sequence", "text sequence"],
        "gru": ["gru", "gated recurrent"],
        "transformer": ["transformer", "attention mechanism", "self-attention"],
        "gpt": ["gpt", "language model", "text generation", "generate text"],

        # Classical ML
        "random_forest": ["random forest", "forest classifier", "ensemble tree"],
        "xgboost": ["xgboost", "xg boost", "gradient boost", "boosted tree"],
        "logistic_regression": ["logistic regression", "logistic", "binary classification"],
        "svm": ["svm", "support vector", "kernel"],
        "decision_tree": ["decision tree", "tree classifier"],
        "knn": ["knn", "k-nearest", "nearest neighbor"],
        "naive_bayes": ["naive bayes", "bayes classifier"],

        # Time series
        "lstm_timeseries": ["time series", "forecasting", "predict future", "stock prediction"],
        "prophet": ["prophet", "facebook prophet"],
        "arima": ["arima", "autoregressive"],

        # Generative
        "gan": ["gan", "generative adversarial", "generate images"],
        "vae": ["vae", "variational autoencoder"],

        # Clustering
        "kmeans": ["kmeans", "k-means", "clustering", "cluster data", "unsupervised"],
    }

    detected_model = "cnn"  # Default
    max_score = 0

    for model, patterns in model_patterns.items():
        score = sum(1 for p in patterns if p in prompt_lower)
        if score > max_score:
            max_score = score
            detected_model = model

    # ==================== Detect Category ====================

    category_patterns = {
        "computer_vision": ["image", "picture", "photo", "visual", "vision", "cnn", "resnet", "detect", "segmentation"],
        "nlp": ["text", "language", "sentiment", "nlp", "words", "document", "bert", "gpt", "translation"],
        "classification": ["classify", "classification", "predict class", "categorize", "labels"],
        "regression": ["regression", "predict value", "continuous", "price prediction", "numeric prediction"],
        "time_series": ["time series", "forecast", "temporal", "stock", "weather prediction", "future"],
        "generative": ["generate", "create images", "gan", "vae", "synthesis"],
        "recommendation": ["recommend", "recommendation", "suggest", "collaborative filtering"],
        "clustering": ["cluster", "grouping", "unsupervised", "segment customers"],
    }

    detected_category = "classification"  # Default
    max_cat_score = 0

    for category, patterns in category_patterns.items():
        score = sum(1 for p in patterns if p in prompt_lower)
        if score > max_cat_score:
            max_cat_score = score
            detected_category = category

    # ==================== Detect Data Type ====================

    data_type = "unknown"
    if any(kw in prompt_lower for kw in ["image", "picture", "photo", "visual", "jpg", "png", "folder"]):
        data_type = "image"
    elif any(kw in prompt_lower for kw in ["csv", "tabular", "spreadsheet", "columns", "rows", "excel"]):
        data_type = "tabular"
    elif any(kw in prompt_lower for kw in ["text", "document", "sentence", "paragraph", "nlp"]):
        data_type = "text"
    elif any(kw in prompt_lower for kw in ["time series", "temporal", "date", "timestamp"]):
        data_type = "time_series"

    # ==================== Detect Problem Type ====================

    problem_type = "classification"  # Default
    if any(kw in prompt_lower for kw in ["regression", "predict value", "continuous", "price"]):
        problem_type = "regression"
    elif any(kw in prompt_lower for kw in ["detection", "detect object", "bounding box", "yolo"]):
        problem_type = "detection"
    elif any(kw in prompt_lower for kw in ["segment", "mask", "pixel-wise"]):
        problem_type = "segmentation"
    elif any(kw in prompt_lower for kw in ["cluster", "grouping", "unsupervised"]):
        problem_type = "clustering"
    elif any(kw in prompt_lower for kw in ["generate", "synthesis", "create new"]):
        problem_type = "generation"

    # ==================== Extract Configuration ====================

    extracted_config = {}
    extracted_features = []

    # Extract num_classes
    class_patterns = [
        r'(\d+)\s*class(?:es)?',
        r'(\d+)\s*categor(?:y|ies)',
        r'(\d+)\s*labels?',
        r'(\d+)\s*types?\s+of',
    ]
    for pattern in class_patterns:
        match = re.search(pattern, prompt_lower)
        if match:
            extracted_config["num_classes"] = int(match.group(1))
            break

    # Extract input_size
    size_patterns = [
        r'(\d+)\s*x\s*\d+\s*(?:px|pixels?|resolution)?',
        r'(\d+)\s*(?:px|pixels?)\s*input',
        r'input\s*size\s*(\d+)',
    ]
    for pattern in size_patterns:
        match = re.search(pattern, prompt_lower)
        if match:
            extracted_config["input_size"] = int(match.group(1))
            break

    # Extract epochs
    epoch_match = re.search(r'(\d+)\s*epochs?', prompt_lower)
    if epoch_match:
        extracted_config["epochs"] = int(epoch_match.group(1))

    # Extract batch_size
    batch_match = re.search(r'batch\s*(?:size)?\s*(?:of)?\s*(\d+)', prompt_lower)
    if batch_match:
        extracted_config["batch_size"] = int(batch_match.group(1))

    # Extract learning_rate
    lr_match = re.search(r'learning\s*rate\s*(?:of)?\s*([\d.]+)', prompt_lower)
    if lr_match:
        extracted_config["learning_rate"] = float(lr_match.group(1))

    # ==================== Extract Features/Requirements ====================

    feature_keywords = {
        "data augmentation": ["augmentation", "augment", "flip", "rotate"],
        "early stopping": ["early stopping", "stop early"],
        "dropout": ["dropout", "drop out"],
        "batch normalization": ["batch norm", "batchnorm"],
        "learning rate scheduler": ["lr scheduler", "learning rate schedule", "reduce lr"],
        "cross validation": ["cross validation", "k-fold", "cv"],
        "hyperparameter tuning": ["hyperparameter", "grid search", "random search", "optuna"],
        "model checkpoint": ["checkpoint", "save model", "save best"],
        "tensorboard logging": ["tensorboard", "logging", "visualize training"],
        "confusion matrix": ["confusion matrix", "classification report"],
        "ROC curve": ["roc curve", "auc"],
        "transfer learning": ["transfer learning", "pretrained", "fine-tune", "finetune"],
        "class weights": ["class weight", "imbalanced", "weighted loss"],
        "GPU training": ["gpu", "cuda", "accelerate"],
    }

    for feature, keywords in feature_keywords.items():
        if any(kw in prompt_lower for kw in keywords):
            extracted_features.append(feature)

    # ==================== Calculate Confidence ====================

    confidence = min(1.0, (max_score + max_cat_score) / 6.0 + 0.3)
    if extracted_config:
        confidence = min(1.0, confidence + 0.1)
    if extracted_features:
        confidence = min(1.0, confidence + 0.1)

    # ==================== Suggest Alternative Models ====================

    suggested_models = []
    if detected_category == "computer_vision":
        suggested_models = ["cnn", "resnet", "efficientnet", "mobilenet"]
    elif detected_category == "nlp":
        suggested_models = ["bert", "lstm", "transformer"]
    elif detected_category == "classification" and data_type == "tabular":
        suggested_models = ["random_forest", "xgboost", "logistic_regression", "svm"]
    elif detected_category == "regression":
        suggested_models = ["xgboost", "random_forest", "linear_regression"]
    elif detected_category == "time_series":
        suggested_models = ["lstm_timeseries", "prophet", "arima"]

    # Remove detected model from suggestions
    suggested_models = [m for m in suggested_models if m != detected_model][:3]

    return PromptAnalysisResult(
        detected_model_type=detected_model,
        detected_category=detected_category,
        confidence=confidence,
        extracted_config=extracted_config,
        extracted_features=extracted_features,
        suggested_models=suggested_models,
        data_type=data_type,
        problem_type=problem_type
    )


@router.post("/analyze-prompt", response_model=PromptAnalyzeResponse)
async def analyze_prompt(request: PromptAnalyzeRequest):
    """
    Analyze a natural language prompt without generating a project.

    This endpoint helps users understand how their prompt will be interpreted
    and what kind of project will be generated. Use this to preview before
    actually creating the project.

    Example prompts:
    - "Create a CNN to classify plant diseases with 4 classes"
    - "Build an XGBoost model to predict house prices from CSV data"
    - "I want to detect objects in images using YOLO"
    - "Create a sentiment analysis model using BERT"
    """
    analysis = analyze_ml_prompt(request.prompt)

    # Generate suggestions for improving the prompt
    suggestions = []

    if "num_classes" not in analysis.extracted_config:
        suggestions.append("Specify the number of classes (e.g., '4 classes', '10 categories')")

    if analysis.data_type == "unknown":
        suggestions.append("Mention your data type (e.g., 'image dataset', 'CSV file', 'text data')")

    if not analysis.extracted_features:
        suggestions.append("Add requirements like 'data augmentation', 'early stopping', 'cross validation'")

    if analysis.confidence < 0.6:
        suggestions.append("Be more specific about the model type (e.g., 'ResNet', 'XGBoost', 'BERT')")

    # Determine if ready to generate
    ready = analysis.confidence >= 0.5 and analysis.detected_model_type != ""

    return PromptAnalyzeResponse(
        analysis=analysis,
        suggested_prompt_improvements=suggestions,
        ready_to_generate=ready
    )


@router.post("/generate-from-prompt", response_model=PromptMLGenerateResponse)
async def generate_from_prompt(
    request: PromptMLGenerateRequest,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_db)
):
    """
    Generate an ML project from a natural language prompt.

    This endpoint analyzes the prompt, determines the best model type and
    configuration, and generates a complete ML project.

    Example prompts:
    - "Create a CNN to classify 5 types of flowers with data augmentation"
    - "Build a random forest model for predicting customer churn"
    - "I need a YOLO model for detecting cars in traffic images"
    - "Create a sentiment classifier using BERT with 3 classes"

    The system will:
    1. Analyze your prompt to understand requirements
    2. Select the best model type (or use your override)
    3. Extract configuration (num_classes, input_size, etc.)
    4. Generate complete project files
    """
    # Analyze the prompt
    analysis = analyze_ml_prompt(request.prompt)

    # Use override model type if provided
    model_type = request.model_type or analysis.detected_model_type

    # Validate model type
    try:
        ml_model = MLModel(model_type)
    except ValueError:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid model type: {model_type}. Detected: {analysis.detected_model_type}"
        )

    template = ml_templates_service.get_template(ml_model)
    if not template:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Template not found for model: {model_type}"
        )

    # Generate project name if not provided
    project_name = request.project_name or f"{model_type}_project"
    project_name = re.sub(r'[^a-zA-Z0-9_]', '_', project_name)[:50]

    # Build configuration from template defaults + extracted config + overrides
    final_config = {**template.config}

    # Apply extracted configuration
    for key, value in analysis.extracted_config.items():
        final_config[key] = value

    # Apply explicit overrides
    if request.num_classes:
        final_config["num_classes"] = request.num_classes
    if request.input_size:
        final_config["input_size"] = request.input_size

    # Generate files
    files = ml_templates_service.generate_project(ml_model, project_name, final_config)

    # Add extracted features as comments in the generated code
    if analysis.extracted_features and "train.py" in files:
        features_comment = f'''
# ==================== User Requirements ====================
# The following features were requested in the prompt:
# {chr(10).join(f"# - {f}" for f in analysis.extracted_features)}
# TODO: Implement these features based on your requirements
# ===========================================================

'''
        files["train.py"] = features_comment + files["train.py"]

    # Add prompt info to README
    if "README.md" in files:
        prompt_section = f"""

## Generated from Prompt

This project was generated from the following natural language description:

> {request.prompt}

### Detected Configuration
- **Model Type**: {model_type}
- **Category**: {analysis.detected_category}
- **Data Type**: {analysis.data_type}
- **Problem Type**: {analysis.problem_type}
- **Confidence**: {analysis.confidence:.0%}

### Extracted Settings
{chr(10).join(f"- **{k}**: {v}" for k, v in final_config.items())}

### Requested Features
{chr(10).join(f"- {f}" for f in analysis.extracted_features) if analysis.extracted_features else "- None specified"}
"""
        files["README.md"] = files["README.md"] + prompt_section

    # Create project in database
    project = Project(
        user_id=str(current_user.id),
        workspace_id=request.workspace_id,
        title=project_name,
        description=f"ML Project generated from prompt: {request.prompt[:200]}...",
        mode=ProjectMode.DEVELOPER,
        status=ProjectStatus.COMPLETED,
        config={
            "ml_model": model_type,
            "ml_category": template.category.value,
            "ml_framework": template.framework.value,
            "ml_config": final_config,
            "generated_from_prompt": True,
            "original_prompt": request.prompt,
            "prompt_analysis": {
                "detected_model_type": analysis.detected_model_type,
                "detected_category": analysis.detected_category,
                "confidence": analysis.confidence,
                "extracted_features": analysis.extracted_features,
                "data_type": analysis.data_type,
                "problem_type": analysis.problem_type,
            }
        },
        framework=template.framework.value,
        technology=f"Python, {template.framework.value.title()}, ML"
    )
    db.add(project)
    await db.flush()

    # Create project files
    for file_path, content in files.items():
        project_file = ProjectFile(
            project_id=project.id,
            path=file_path,
            name=file_path.split("/")[-1],
            content_inline=content,
            is_inline=True,
            language=get_language_from_filename(file_path),
            size_bytes=len(content.encode('utf-8'))
        )
        db.add(project_file)

    await db.commit()

    logger.info(f"Created ML project from prompt: {project.id} ({model_type}) for user {current_user.id}")

    return PromptMLGenerateResponse(
        project_id=str(project.id),
        project_name=project_name,
        model_type=model_type,
        category=template.category.value,
        framework=template.framework.value,
        files_created=len(files),
        prompt_analysis=analysis,
        message=f"ML project '{project_name}' created successfully from your prompt with {len(files)} files"
    )
