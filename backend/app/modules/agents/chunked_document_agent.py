"""
CHUNKED DOCUMENT GENERATOR AGENT
================================
Generates 60-80 page Word documents and PPTs by breaking into sections.

Architecture:
1. Phase 1: Generate Document Outline (structure + sections)
2. Phase 2: Generate each section content separately (parallel)
3. Phase 3: Generate UML diagrams
4. Phase 4: Assemble into final Word/PPT documents

Features:
- Token limit handling via chunking
- Parallel section generation for speed
- Retry logic for failed sections
- College info integration (Certificate, Declaration, Acknowledgement)
- Dynamic UML diagram generation

This bypasses Claude's token limits by chunking the generation.
"""

from typing import Dict, List, Optional, Any, AsyncGenerator, Tuple, TYPE_CHECKING
from collections import OrderedDict

if TYPE_CHECKING:
    from sqlalchemy.ext.asyncio import AsyncSession
from dataclasses import dataclass, field
import json
import asyncio
from datetime import datetime
from enum import Enum

from app.core.logging_config import logger
from app.modules.agents.base_agent import BaseAgent, AgentContext
from app.modules.automation.uml_generator import uml_generator


@dataclass
class CollegeInfo:
    """College information for academic documents"""
    college_name: str = "College Name"
    affiliated_to: str = "Autonomous Institution"
    college_address: str = ""
    department: str = "Department of Computer Science and Engineering"
    academic_year: str = "2024-2025"
    guide_name: str = "Dr. Guide Name"
    hod_name: str = "Dr. HOD Name"
    principal_name: str = "Dr. Principal Name"
    project_title: str = "Project Title"
    date: str = ""
    students: List[Dict] = field(default_factory=list)  # [{"name": "...", "roll_number": "..."}]

    def __post_init__(self):
        if not self.date:
            self.date = datetime.now().strftime("%B %Y")
        if not self.students:
            self.students = [{"name": "Student Name", "roll_number": "ROLL001"}]

    def to_dict(self) -> Dict:
        return {
            "college_name": self.college_name,
            "affiliated_to": self.affiliated_to,
            "college_address": self.college_address,
            "department": self.department,
            "academic_year": self.academic_year,
            "guide_name": self.guide_name,
            "hod_name": self.hod_name,
            "principal_name": self.principal_name,
            "project_title": self.project_title,
            "date": self.date,
            "students": self.students
        }

    @classmethod
    def from_dict(cls, data: Dict) -> "CollegeInfo":
        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})


class DocumentType(str, Enum):
    """Supported document types"""
    SRS = "srs"
    SDS = "sds"
    PROJECT_REPORT = "project_report"
    PPT = "ppt"
    TESTING_PLAN = "testing_plan"
    USER_MANUAL = "user_manual"
    VIVA_QA = "viva_qa"  # Viva Questions and Answers document



# ============================================================================
# Domain-specific SRS templates - Auto-selected based on project type
# All content is DYNAMICALLY generated by AI based on actual project data
# ============================================================================
SRS_TEMPLATES = {
    "web_application": {
        "name": "Web Application SRS",
        "estimated_pages": 65,
        "domain_keywords": ["web", "website", "portal", "dashboard", "react", "angular", "vue", "nextjs", "frontend", "full-stack", "fullstack", "spa", "single page"],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Scope", "1.3 Definitions", "1.4 References", "1.5 Overview"]},
            {"id": "sec2_overall", "title": "Chapter 2: Overall Description", "pages": 6, "type": "generate", "subsections": ["2.1 Product Perspective", "2.2 Product Functions", "2.3 User Classes", "2.4 Operating Environment", "2.5 Constraints", "2.6 Dependencies"]},
            {"id": "sec3_functional", "title": "Chapter 3: Functional Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 Authentication Module", "3.2 User Management", "3.3 Core Features", "3.4 Search/Filter", "3.5 Data Management", "3.6 Notifications"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 ER Diagram"], "include_diagrams": True},
            {"id": "sec5_ui", "title": "Chapter 5: User Interface Requirements", "pages": 8, "type": "generate", "subsections": ["5.1 UI Design Principles", "5.2 Page Layouts", "5.3 Navigation", "5.4 Responsive Design", "5.5 Accessibility (WCAG)"]},
            {"id": "sec6_api", "title": "Chapter 6: API Requirements", "pages": 6, "type": "generate", "subsections": ["6.1 RESTful Design", "6.2 Endpoints", "6.3 Request/Response", "6.4 JWT/OAuth", "6.5 Error Handling"]},
            {"id": "sec7_nonfunctional", "title": "Chapter 7: Non-Functional Requirements", "pages": 6, "type": "generate", "subsections": ["7.1 Performance", "7.2 Security (OWASP)", "7.3 Browser Compatibility", "7.4 Scalability", "7.5 SEO"]},
            {"id": "sec8_testing", "title": "Chapter 8: Testing Requirements", "pages": 4, "type": "generate", "subsections": ["8.1 Unit Testing", "8.2 Integration Testing", "8.3 E2E Testing", "8.4 Cross-Browser"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
        ]
    },
    "mobile_application": {
        "name": "Mobile Application SRS",
        "estimated_pages": 65,
        "domain_keywords": ["mobile", "android", "ios", "flutter", "react native", "kotlin", "swift", "app store", "play store", "smartphone"],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Scope", "1.3 Target Platforms", "1.4 Definitions", "1.5 Overview"]},
            {"id": "sec2_overall", "title": "Chapter 2: Overall Description", "pages": 6, "type": "generate", "subsections": ["2.1 Product Perspective", "2.2 Functions", "2.3 User Classes", "2.4 Device Requirements", "2.5 OS Requirements", "2.6 Dependencies"]},
            {"id": "sec3_functional", "title": "Chapter 3: Functional Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 Biometric Auth", "3.2 Profile Management", "3.3 Core Features", "3.4 Push Notifications", "3.5 Offline Mode", "3.6 Data Sync"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 ER Diagram"], "include_diagrams": True},
            {"id": "sec5_ui", "title": "Chapter 5: Mobile UI/UX Requirements", "pages": 8, "type": "generate", "subsections": ["5.1 Material/Human Interface", "5.2 Screen Flow", "5.3 Gestures", "5.4 Screen Adaptability", "5.5 Accessibility"]},
            {"id": "sec6_platform", "title": "Chapter 6: Platform-Specific Requirements", "pages": 6, "type": "generate", "subsections": ["6.1 iOS Requirements", "6.2 Android Requirements", "6.3 Cross-Platform", "6.4 Permissions", "6.5 Store Guidelines"]},
            {"id": "sec7_nonfunctional", "title": "Chapter 7: Non-Functional Requirements", "pages": 6, "type": "generate", "subsections": ["7.1 Performance", "7.2 Battery Optimization", "7.3 Network Efficiency", "7.4 Security", "7.5 App Size"]},
            {"id": "sec8_testing", "title": "Chapter 8: Testing Requirements", "pages": 4, "type": "generate", "subsections": ["8.1 Device Matrix", "8.2 UI Testing", "8.3 Performance Testing", "8.4 Beta Testing"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
        ]
    },
    "ml_ai_project": {
        "name": "ML/AI Project SRS",
        "estimated_pages": 65,
        "domain_keywords": ["machine learning", "ml", "ai", "artificial intelligence", "deep learning", "neural network", "tensorflow", "pytorch", "model", "prediction", "classification", "nlp", "computer vision", "data science"],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Problem Statement", "1.3 Objectives", "1.4 Definitions", "1.5 Overview"]},
            {"id": "sec2_dataset", "title": "Chapter 2: Dataset Requirements", "pages": 8, "type": "generate", "subsections": ["2.1 Data Source", "2.2 Collection Methodology", "2.3 Data Format", "2.4 Quality Criteria", "2.5 Preprocessing", "2.6 Train/Val/Test Split"]},
            {"id": "sec3_model", "title": "Chapter 3: Model Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 Architecture Selection", "3.2 Algorithm Comparison", "3.3 Hyperparameters", "3.4 Training Requirements", "3.5 Feature Engineering"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 Data Flow Diagram"], "include_diagrams": True},
            {"id": "sec5_pipeline", "title": "Chapter 5: ML Pipeline", "pages": 6, "type": "generate", "subsections": ["5.1 Data Pipeline", "5.2 Training Pipeline", "5.3 Model Versioning", "5.4 Experiment Tracking", "5.5 CI/CD for ML"]},
            {"id": "sec6_deployment", "title": "Chapter 6: Deployment", "pages": 6, "type": "generate", "subsections": ["6.1 Architecture", "6.2 Model Serving", "6.3 API Design", "6.4 Batch vs Real-time", "6.5 Monitoring"]},
            {"id": "sec7_nonfunctional", "title": "Chapter 7: Non-Functional Requirements", "pages": 5, "type": "generate", "subsections": ["7.1 Performance Metrics", "7.2 Scalability", "7.3 Security", "7.4 Ethical Considerations", "7.5 Explainability"]},
            {"id": "sec8_testing", "title": "Chapter 8: Testing", "pages": 4, "type": "generate", "subsections": ["8.1 Validation Strategy", "8.2 Edge Cases", "8.3 Adversarial Testing", "8.4 Production Monitoring"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
        ]
    },
    "ecommerce": {
        "name": "E-Commerce SRS",
        "estimated_pages": 65,
        "domain_keywords": ["ecommerce", "e-commerce", "shopping", "cart", "checkout", "payment", "store", "marketplace", "product", "inventory", "order", "shop"],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Business Objectives", "1.3 Scope", "1.4 Definitions", "1.5 Overview"]},
            {"id": "sec2_overall", "title": "Chapter 2: Overall Description", "pages": 6, "type": "generate", "subsections": ["2.1 Product Perspective", "2.2 User Classes (Customer/Vendor/Admin)", "2.3 Environment", "2.4 Constraints", "2.5 Dependencies"]},
            {"id": "sec3_functional", "title": "Chapter 3: Functional Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 Product Catalog", "3.2 Shopping Cart", "3.3 Checkout Flow", "3.4 Order Management", "3.5 User Accounts"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 ER Diagram"], "include_diagrams": True},
            {"id": "sec5_payment", "title": "Chapter 5: Payment and Security", "pages": 6, "type": "generate", "subsections": ["5.1 Payment Gateway", "5.2 Payment Methods", "5.3 PCI DSS Compliance", "5.4 Refunds", "5.5 Fraud Detection"]},
            {"id": "sec6_order", "title": "Chapter 6: Order Management", "pages": 6, "type": "generate", "subsections": ["6.1 Order Processing", "6.2 Status Tracking", "6.3 Shipping Integration", "6.4 Returns/Exchange", "6.5 Notifications"]},
            {"id": "sec7_admin", "title": "Chapter 7: Admin Panel", "pages": 5, "type": "generate", "subsections": ["7.1 Admin Dashboard", "7.2 Sales Analytics", "7.3 Customer Management", "7.4 Inventory Management", "7.5 Reports"]},
            {"id": "sec8_nonfunctional", "title": "Chapter 8: Non-Functional Requirements", "pages": 5, "type": "generate", "subsections": ["8.1 Performance", "8.2 Security", "8.3 Scalability", "8.4 SEO", "8.5 Mobile Responsive"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
        ]
    },
    "healthcare": {
        "name": "Healthcare Application SRS",
        "estimated_pages": 65,
        "domain_keywords": ["healthcare", "health", "medical", "hospital", "patient", "doctor", "clinic", "diagnosis", "pharmacy", "telemedicine", "ehr", "emr", "hipaa"],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Healthcare Context", "1.3 Scope", "1.4 Medical Terminology", "1.5 Overview"]},
            {"id": "sec2_overall", "title": "Chapter 2: Overall Description", "pages": 6, "type": "generate", "subsections": ["2.1 Product Perspective", "2.2 User Classes", "2.3 Operating Environment", "2.4 Constraints", "2.5 Dependencies"]},
            {"id": "sec3_functional", "title": "Chapter 3: Functional Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 Patient Registration", "3.2 EHR Management", "3.3 Appointments", "3.4 Clinical Documentation", "3.5 Prescriptions"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 ER Diagram"], "include_diagrams": True},
            {"id": "sec5_compliance", "title": "Chapter 5: Regulatory Compliance", "pages": 6, "type": "generate", "subsections": ["5.1 HIPAA Compliance", "5.2 HL7/FHIR Standards", "5.3 Data Privacy", "5.4 Audit Trail", "5.5 Consent Management"]},
            {"id": "sec6_security", "title": "Chapter 6: Security Requirements", "pages": 6, "type": "generate", "subsections": ["6.1 Authentication (MFA)", "6.2 Role-Based Access", "6.3 Encryption", "6.4 Secure Transmission", "6.5 Incident Response"]},
            {"id": "sec7_integration", "title": "Chapter 7: Integration Requirements", "pages": 5, "type": "generate", "subsections": ["7.1 Hospital Information System", "7.2 Lab Integration", "7.3 Pharmacy Systems", "7.4 Insurance APIs", "7.5 Telemedicine"]},
            {"id": "sec8_testing", "title": "Chapter 8: Testing Requirements", "pages": 4, "type": "generate", "subsections": ["8.1 Clinical Validation", "8.2 Security Testing", "8.3 Compliance Audit", "8.4 User Acceptance Testing"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
        ]
    },
    "iot_project": {
        "name": "IoT Project SRS",
        "estimated_pages": 65,
        "domain_keywords": ["iot", "internet of things", "sensor", "embedded", "arduino", "raspberry pi", "mqtt", "hardware", "smart home", "automation", "microcontroller", "device"],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Project Overview", "1.3 System Context", "1.4 Definitions", "1.5 Overview"]},
            {"id": "sec2_hardware", "title": "Chapter 2: Hardware Requirements", "pages": 8, "type": "generate", "subsections": ["2.1 Microcontroller", "2.2 Sensors", "2.3 Actuators", "2.4 Power Supply", "2.5 Connectivity Modules"]},
            {"id": "sec3_functional", "title": "Chapter 3: Functional Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 Data Acquisition", "3.2 Edge Processing", "3.3 Communication Protocol", "3.4 Cloud Integration", "3.5 User Interface"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 Component Diagram"], "include_diagrams": True},
            {"id": "sec5_firmware", "title": "Chapter 5: Firmware Requirements", "pages": 6, "type": "generate", "subsections": ["5.1 OS/RTOS Selection", "5.2 Boot Process", "5.3 OTA Updates", "5.4 Power Management", "5.5 Error Handling"]},
            {"id": "sec6_cloud", "title": "Chapter 6: Cloud/Backend", "pages": 6, "type": "generate", "subsections": ["6.1 IoT Platform", "6.2 Device Registry", "6.3 Data Processing", "6.4 Storage", "6.5 Dashboard"]},
            {"id": "sec7_security", "title": "Chapter 7: Security Requirements", "pages": 5, "type": "generate", "subsections": ["7.1 Device Authentication", "7.2 Secure Communication", "7.3 Firmware Security", "7.4 Access Control", "7.5 Intrusion Detection"]},
            {"id": "sec8_testing", "title": "Chapter 8: Testing Requirements", "pages": 4, "type": "generate", "subsections": ["8.1 Hardware Testing", "8.2 Firmware Testing", "8.3 Integration Testing", "8.4 Field Testing"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
        ]
    },
    "default": {
        "name": "Software Requirements Specification",
        "estimated_pages": 65,
        "domain_keywords": [],
        "sections": [
            {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
            {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
            {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
            {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
            {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
            {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
            {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
            {"id": "list_tables", "title": "List of Tables", "pages": 1, "type": "auto"},
            {"id": "sec1_intro", "title": "Chapter 1: Introduction", "pages": 5, "type": "generate", "subsections": ["1.1 Purpose", "1.2 Scope", "1.3 Definitions and Acronyms", "1.4 References", "1.5 Document Overview"]},
            {"id": "sec2_overall", "title": "Chapter 2: Overall Description", "pages": 6, "type": "generate", "subsections": ["2.1 Product Perspective", "2.2 Product Functions", "2.3 User Classes and Characteristics", "2.4 Operating Environment", "2.5 Design Constraints", "2.6 Assumptions and Dependencies"]},
            {"id": "sec3_requirements", "title": "Chapter 3: Functional Requirements", "pages": 8, "type": "generate", "subsections": ["3.1 User Authentication Module", "3.2 Core Business Module", "3.3 Data Management Module", "3.4 Reporting Module", "3.5 Admin Module"]},
            {"id": "sec4_uml", "title": "Chapter 4: System Design (UML Diagrams)", "pages": 10, "type": "generate", "subsections": ["4.1 Use Case Diagram", "4.2 Class Diagram", "4.3 Sequence Diagram", "4.4 Activity Diagram", "4.5 ER Diagram"], "include_diagrams": True},
            {"id": "sec5_interface", "title": "Chapter 5: External Interface Requirements", "pages": 6, "type": "generate", "subsections": ["5.1 User Interfaces", "5.2 Hardware Interfaces", "5.3 Software Interfaces", "5.4 Communication Interfaces"]},
            {"id": "sec6_nonfunctional", "title": "Chapter 6: Non-Functional Requirements", "pages": 6, "type": "generate", "subsections": ["6.1 Performance Requirements", "6.2 Security Requirements", "6.3 Reliability Requirements", "6.4 Scalability Requirements", "6.5 Usability Requirements"]},
            {"id": "sec7_database", "title": "Chapter 7: Database Design", "pages": 5, "type": "generate", "subsections": ["7.1 Database Overview", "7.2 Table Structures", "7.3 Data Dictionary", "7.4 Database Constraints"]},
            {"id": "sec8_testing", "title": "Chapter 8: Testing Requirements", "pages": 4, "type": "generate", "subsections": ["8.1 Testing Strategy", "8.2 Test Cases", "8.3 Quality Assurance"]},
            {"id": "sec9_conclusion", "title": "Chapter 9: Conclusion", "pages": 2, "type": "generate", "subsections": ["9.1 Summary", "9.2 Future Enhancements"]},
            {"id": "references", "title": "References", "pages": 2, "type": "generate"},
            {"id": "appendix_a", "title": "Appendix A: Glossary", "pages": 2, "type": "generate"},
        ]
    }
}


def _detect_project_domain(project_data: dict) -> str:
    """Auto-detect project domain based on project_type, technologies, and features."""
    search_text = " ".join([
        str(project_data.get("project_name", "")).lower(),
        str(project_data.get("project_type", "")).lower(),
        " ".join([str(t).lower() for t in project_data.get("technologies", {}).keys()]) if isinstance(project_data.get("technologies"), dict) else str(project_data.get("technologies", "")).lower(),
        " ".join([str(f).lower() for f in project_data.get("features", [])]) if isinstance(project_data.get("features"), list) else str(project_data.get("features", "")).lower(),
    ])

    domain_scores = {}
    for domain_key, template in SRS_TEMPLATES.items():
        if domain_key == "default":
            continue
        score = sum(1 for kw in template.get("domain_keywords", []) if kw.lower() in search_text)
        if score > 0:
            domain_scores[domain_key] = score

    return max(domain_scores, key=domain_scores.get) if domain_scores else "default"


def _get_srs_structure(project_data: dict) -> dict:
    """Get the appropriate SRS structure based on detected project domain."""
    domain = _detect_project_domain(project_data)
    template = SRS_TEMPLATES.get(domain, SRS_TEMPLATES["default"])
    return {
        "name": template["name"],
        "estimated_pages": template["estimated_pages"],
        "sections": template["sections"],
        "detected_domain": domain
    }




# ============================================================================
# Domain-specific PPT templates - Auto-selected based on project type
# All content is DYNAMICALLY generated by AI based on actual project data
# ============================================================================
PPT_TEMPLATES = {
    "web_application": {
        "name": "Web Application Presentation",
        "estimated_slides": 25,
        "domain_keywords": ["web", "website", "portal", "dashboard", "react", "angular", "vue", "nextjs", "frontend", "full-stack", "fullstack"],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "Project Introduction", "slides": 2, "type": "generate"},
            {"id": "problem", "title": "Problem Statement", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Objectives", "slides": 2, "type": "generate"},
            {"id": "tech_stack", "title": "Technology Stack", "slides": 2, "type": "generate"},
            {"id": "architecture", "title": "System Architecture", "slides": 2, "type": "generate"},
            {"id": "ui_design", "title": "UI/UX Design", "slides": 3, "type": "generate"},
            {"id": "features", "title": "Key Features", "slides": 3, "type": "generate"},
            {"id": "api_design", "title": "API Design", "slides": 2, "type": "generate"},
            {"id": "demo", "title": "Live Demo Screenshots", "slides": 3, "type": "generate"},
            {"id": "testing", "title": "Testing Results", "slides": 2, "type": "generate"},
            {"id": "deployment", "title": "Deployment", "slides": 1, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Enhancements", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You / Q&A", "slides": 1, "type": "template"},
        ]
    },
    "mobile_application": {
        "name": "Mobile App Presentation",
        "estimated_slides": 25,
        "domain_keywords": ["mobile", "android", "ios", "flutter", "react native", "kotlin", "swift"],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "App Introduction", "slides": 2, "type": "generate"},
            {"id": "problem", "title": "Problem Statement", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Objectives", "slides": 2, "type": "generate"},
            {"id": "platforms", "title": "Target Platforms", "slides": 1, "type": "generate"},
            {"id": "tech_stack", "title": "Technology Stack", "slides": 2, "type": "generate"},
            {"id": "architecture", "title": "App Architecture", "slides": 2, "type": "generate"},
            {"id": "ui_screens", "title": "UI/UX Screens", "slides": 4, "type": "generate"},
            {"id": "features", "title": "Core Features", "slides": 3, "type": "generate"},
            {"id": "offline", "title": "Offline Capability", "slides": 1, "type": "generate"},
            {"id": "demo", "title": "App Demo", "slides": 3, "type": "generate"},
            {"id": "testing", "title": "Testing on Devices", "slides": 2, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Updates", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You / Q&A", "slides": 1, "type": "template"},
        ]
    },
    "ml_ai_project": {
        "name": "ML/AI Project Presentation",
        "estimated_slides": 25,
        "domain_keywords": ["machine learning", "ml", "ai", "artificial intelligence", "deep learning", "neural network", "tensorflow", "pytorch", "model", "prediction"],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "Project Introduction", "slides": 2, "type": "generate"},
            {"id": "problem", "title": "Problem Statement", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Research Objectives", "slides": 2, "type": "generate"},
            {"id": "dataset", "title": "Dataset Analysis", "slides": 3, "type": "generate"},
            {"id": "methodology", "title": "Methodology", "slides": 2, "type": "generate"},
            {"id": "model", "title": "Model Architecture", "slides": 3, "type": "generate"},
            {"id": "training", "title": "Training Process", "slides": 2, "type": "generate"},
            {"id": "results", "title": "Results & Metrics", "slides": 3, "type": "generate"},
            {"id": "comparison", "title": "Model Comparison", "slides": 2, "type": "generate"},
            {"id": "demo", "title": "Demo / Predictions", "slides": 2, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Work", "slides": 1, "type": "generate"},
            {"id": "references", "title": "References", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You / Q&A", "slides": 1, "type": "template"},
        ]
    },
    "ecommerce": {
        "name": "E-Commerce Presentation",
        "estimated_slides": 25,
        "domain_keywords": ["ecommerce", "e-commerce", "shopping", "cart", "checkout", "payment", "store", "marketplace"],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "Platform Introduction", "slides": 2, "type": "generate"},
            {"id": "problem", "title": "Market Problem", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Business Objectives", "slides": 2, "type": "generate"},
            {"id": "tech_stack", "title": "Technology Stack", "slides": 2, "type": "generate"},
            {"id": "architecture", "title": "System Architecture", "slides": 2, "type": "generate"},
            {"id": "product_mgmt", "title": "Product Management", "slides": 2, "type": "generate"},
            {"id": "cart_checkout", "title": "Cart & Checkout Flow", "slides": 2, "type": "generate"},
            {"id": "payment", "title": "Payment Integration", "slides": 2, "type": "generate"},
            {"id": "admin", "title": "Admin Dashboard", "slides": 2, "type": "generate"},
            {"id": "demo", "title": "Platform Demo", "slides": 3, "type": "generate"},
            {"id": "security", "title": "Security Measures", "slides": 1, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Roadmap", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You / Q&A", "slides": 1, "type": "template"},
        ]
    },
    "healthcare": {
        "name": "Healthcare Application Presentation",
        "estimated_slides": 25,
        "domain_keywords": ["healthcare", "health", "medical", "hospital", "patient", "doctor", "clinic", "diagnosis"],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "System Introduction", "slides": 2, "type": "generate"},
            {"id": "problem", "title": "Healthcare Challenge", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Objectives", "slides": 2, "type": "generate"},
            {"id": "compliance", "title": "Compliance & Standards", "slides": 2, "type": "generate"},
            {"id": "tech_stack", "title": "Technology Stack", "slides": 2, "type": "generate"},
            {"id": "architecture", "title": "System Architecture", "slides": 2, "type": "generate"},
            {"id": "patient_mgmt", "title": "Patient Management", "slides": 2, "type": "generate"},
            {"id": "clinical", "title": "Clinical Features", "slides": 3, "type": "generate"},
            {"id": "security", "title": "Security & Privacy", "slides": 2, "type": "generate"},
            {"id": "demo", "title": "System Demo", "slides": 3, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Enhancements", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You / Q&A", "slides": 1, "type": "template"},
        ]
    },
    "iot_project": {
        "name": "IoT Project Presentation",
        "estimated_slides": 25,
        "domain_keywords": ["iot", "internet of things", "sensor", "embedded", "arduino", "raspberry pi", "mqtt", "hardware", "smart home"],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "Project Introduction", "slides": 2, "type": "generate"},
            {"id": "problem", "title": "Problem Statement", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Objectives", "slides": 2, "type": "generate"},
            {"id": "hardware", "title": "Hardware Components", "slides": 3, "type": "generate"},
            {"id": "circuit", "title": "Circuit Design", "slides": 2, "type": "generate"},
            {"id": "firmware", "title": "Firmware/Software", "slides": 2, "type": "generate"},
            {"id": "connectivity", "title": "Connectivity & Protocols", "slides": 2, "type": "generate"},
            {"id": "cloud", "title": "Cloud Integration", "slides": 2, "type": "generate"},
            {"id": "dashboard", "title": "Dashboard & Monitoring", "slides": 2, "type": "generate"},
            {"id": "demo", "title": "Working Demo", "slides": 3, "type": "generate"},
            {"id": "testing", "title": "Testing Results", "slides": 1, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Scope", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You / Q&A", "slides": 1, "type": "template"},
        ]
    },
    "default": {
        "name": "Project Presentation",
        "estimated_slides": 25,
        "domain_keywords": [],
        "sections": [
            {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
            {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
            {"id": "intro", "title": "Introduction", "slides": 3, "type": "generate"},
            {"id": "problem", "title": "Problem Statement", "slides": 2, "type": "generate"},
            {"id": "objectives", "title": "Objectives", "slides": 2, "type": "generate"},
            {"id": "literature", "title": "Literature Review", "slides": 2, "type": "generate"},
            {"id": "methodology", "title": "Methodology", "slides": 2, "type": "generate"},
            {"id": "architecture", "title": "System Architecture", "slides": 2, "type": "generate"},
            {"id": "implementation", "title": "Implementation", "slides": 3, "type": "generate"},
            {"id": "demo", "title": "Demo Screenshots", "slides": 3, "type": "generate"},
            {"id": "testing", "title": "Testing Results", "slides": 2, "type": "generate"},
            {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
            {"id": "future", "title": "Future Scope", "slides": 1, "type": "generate"},
            {"id": "references", "title": "References", "slides": 1, "type": "generate"},
            {"id": "thankyou", "title": "Thank You", "slides": 1, "type": "template"},
        ]
    }
}


def _get_ppt_structure(project_data: dict) -> dict:
    """Get the appropriate PPT structure based on detected project domain."""
    domain = _detect_project_domain(project_data)  # Reuse SRS domain detection
    template = PPT_TEMPLATES.get(domain, PPT_TEMPLATES["default"])
    return {
        "name": template["name"],
        "estimated_slides": template["estimated_slides"],
        "sections": template["sections"],
        "detected_domain": domain
    }



class ChunkedDocumentAgent(BaseAgent):
    """
    Chunked Document Generator - Handles large documents (60-80 pages)

    Strategy:
    - Break document into logical sections
    - Generate each section with separate Claude calls
    - Assemble final document with python-docx/python-pptx
    """

    # Document structure templates (60-80 pages MAX)
    DOCUMENT_STRUCTURES = {
        DocumentType.PROJECT_REPORT: {
            "name": "Project Report",
            "estimated_pages": 65,  # Target 60-80 pages
            "sections": [
                {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
                {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
                {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
                {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
                {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
                {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
                {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
                {"id": "list_tables", "title": "List of Tables", "pages": 1, "type": "auto"},
                {"id": "ch1_intro", "title": "Chapter 1: Introduction", "pages": 6, "type": "generate", "subsections": [
                    "1.1 Background",
                    "1.2 Problem Statement",
                    "1.3 Objectives",
                    "1.4 Scope of the Project",
                    "1.5 Methodology",
                    "1.6 Organization of Report"
                ]},
                {"id": "ch2_literature", "title": "Chapter 2: Literature Review", "pages": 6, "type": "generate", "subsections": [
                    "2.1 Existing Systems",
                    "2.2 Comparative Analysis",
                    "2.3 Technology Review",
                    "2.4 Gap Analysis"
                ]},
                {"id": "ch3_requirements", "title": "Chapter 3: Requirement Analysis", "pages": 8, "type": "generate", "subsections": [
                    "3.1 Functional Requirements",
                    "3.2 Non-Functional Requirements",
                    "3.3 Hardware Requirements",
                    "3.4 Software Requirements",
                    "3.5 Use Case Diagrams",
                    "3.6 Data Flow Diagrams"
                ]},
                {"id": "ch4_design", "title": "Chapter 4: System Design", "pages": 10, "type": "generate", "subsections": [
                    "4.1 System Architecture",
                    "4.2 Database Design",
                    "4.3 ER Diagram",
                    "4.4 Class Diagram",
                    "4.5 Sequence Diagrams",
                    "4.6 Activity Diagrams",
                    "4.7 API Design",
                    "4.8 UI/UX Design"
                ]},
                {"id": "ch5_implementation", "title": "Chapter 5: Implementation", "pages": 8, "type": "generate", "subsections": [
                    "5.1 Development Environment",
                    "5.2 Frontend Implementation",
                    "5.3 Backend Implementation",
                    "5.4 Database Implementation",
                    "5.5 Code Snippets"
                ]},
                {"id": "ch6_testing", "title": "Chapter 6: Testing", "pages": 6, "type": "generate", "subsections": [
                    "6.1 Testing Strategy",
                    "6.2 Unit Testing",
                    "6.3 Integration Testing",
                    "6.4 System Testing",
                    "6.5 Test Cases and Results"
                ]},
                {"id": "ch7_results", "title": "Chapter 7: Results and Discussion", "pages": 5, "type": "generate", "subsections": [
                    "7.1 Screenshots",
                    "7.2 Performance Analysis",
                    "7.3 User Feedback"
                ]},
                {"id": "ch8_conclusion", "title": "Chapter 8: Conclusion and Future Scope", "pages": 3, "type": "generate", "subsections": [
                    "8.1 Conclusion",
                    "8.2 Limitations",
                    "8.3 Future Enhancements"
                ]},
                {"id": "references", "title": "References", "pages": 2, "type": "generate"},
                {"id": "appendix_a", "title": "Appendix A: Source Code", "pages": 4, "type": "code"},
                {"id": "appendix_b", "title": "Appendix B: Database Schema", "pages": 2, "type": "generate"},
            ]
        },
        DocumentType.SRS: {
            "name": "Software Requirements Specification",
            "estimated_pages": 70,  # Target 60-80 pages
            "sections": [
                {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
                {"id": "certificate", "title": "Certificate", "pages": 1, "type": "template"},
                {"id": "declaration", "title": "Declaration", "pages": 1, "type": "template"},
                {"id": "acknowledgement", "title": "Acknowledgement", "pages": 1, "type": "generate"},
                {"id": "abstract", "title": "Abstract", "pages": 1, "type": "generate"},
                {"id": "toc", "title": "Table of Contents", "pages": 2, "type": "auto"},
                {"id": "list_figures", "title": "List of Figures", "pages": 1, "type": "auto"},
                {"id": "list_tables", "title": "List of Tables", "pages": 1, "type": "auto"},
                {"id": "sec1_intro", "title": "1. Introduction", "pages": 6, "type": "generate", "subsections": [
                    "1.1 Purpose of the Document",
                    "1.2 Project Scope",
                    "1.3 Definitions, Acronyms and Abbreviations",
                    "1.4 References",
                    "1.5 Document Overview",
                    "1.6 Project Background"
                ]},
                {"id": "sec2_overall", "title": "2. Overall Description", "pages": 8, "type": "generate", "subsections": [
                    "2.1 Product Perspective",
                    "2.2 Product Functions Overview",
                    "2.3 User Classes and Characteristics",
                    "2.4 Operating Environment",
                    "2.5 Design and Implementation Constraints",
                    "2.6 User Documentation",
                    "2.7 Assumptions and Dependencies"
                ]},
                {"id": "sec3_requirements", "title": "3. System Features and Requirements", "pages": 12, "type": "generate", "subsections": [
                    "3.1 User Management Module",
                    "3.2 Core Functional Requirements",
                    "3.3 Data Management Requirements",
                    "3.4 Reporting and Analytics",
                    "3.5 Notification System",
                    "3.6 Search and Filter Features"
                ]},
                {"id": "sec4_interface", "title": "4. External Interface Requirements", "pages": 8, "type": "generate", "subsections": [
                    "4.1 User Interface Requirements",
                    "4.2 Hardware Interface Requirements",
                    "4.3 Software Interface Requirements",
                    "4.4 Communication Interface Requirements",
                    "4.5 API Interface Specifications"
                ]},
                {"id": "sec5_nonfunctional", "title": "5. Non-Functional Requirements", "pages": 8, "type": "generate", "subsections": [
                    "5.1 Performance Requirements",
                    "5.2 Security Requirements",
                    "5.3 Reliability and Availability",
                    "5.4 Scalability Requirements",
                    "5.5 Usability Requirements",
                    "5.6 Maintainability Requirements"
                ]},
                {"id": "sec6_database", "title": "6. Database Requirements", "pages": 6, "type": "generate", "subsections": [
                    "6.1 Database Design Overview",
                    "6.2 Entity Relationship Diagram",
                    "6.3 Data Dictionary",
                    "6.4 Database Constraints",
                    "6.5 Data Migration Requirements"
                ]},
                {"id": "sec7_usecase", "title": "7. Use Case Specifications", "pages": 8, "type": "generate", "subsections": [
                    "7.1 Use Case Diagram",
                    "7.2 Actor Descriptions",
                    "7.3 Primary Use Cases",
                    "7.4 Secondary Use Cases",
                    "7.5 Use Case Narratives"
                ]},
                {"id": "sec8_validation", "title": "8. Verification and Validation", "pages": 4, "type": "generate", "subsections": [
                    "8.1 Verification Methods",
                    "8.2 Validation Criteria",
                    "8.3 Acceptance Test Cases",
                    "8.4 Quality Assurance Plan"
                ]},
                {"id": "appendix_a", "title": "Appendix A: Glossary", "pages": 2, "type": "generate"},
                {"id": "appendix_b", "title": "Appendix B: Analysis Models", "pages": 3, "type": "generate"},
                {"id": "appendix_c", "title": "Appendix C: Issues List", "pages": 2, "type": "generate"},
            ]
        },
        DocumentType.SDS: {
            "name": "Software Design Specification",
            "estimated_pages": 30,
            "sections": [
                {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
                {"id": "revision_history", "title": "Revision History", "pages": 1, "type": "template"},
                {"id": "toc", "title": "Table of Contents", "pages": 1, "type": "auto"},
                {"id": "sec1_intro", "title": "1. Introduction", "pages": 2, "type": "generate", "subsections": [
                    "1.1 Purpose",
                    "1.2 Scope",
                    "1.3 Definitions and Acronyms",
                    "1.4 References",
                    "1.5 Document Overview"
                ]},
                {"id": "sec2_architecture", "title": "2. System Architecture", "pages": 6, "type": "generate", "subsections": [
                    "2.1 Architecture Overview",
                    "2.2 Component Diagram",
                    "2.3 Deployment Architecture",
                    "2.4 Technology Stack",
                    "2.5 Design Patterns Used"
                ]},
                {"id": "sec3_database", "title": "3. Database Design", "pages": 5, "type": "generate", "subsections": [
                    "3.1 Database Schema",
                    "3.2 Entity Relationship Diagram",
                    "3.3 Table Descriptions",
                    "3.4 Data Dictionary",
                    "3.5 Database Constraints"
                ]},
                {"id": "sec4_api", "title": "4. API Design", "pages": 5, "type": "generate", "subsections": [
                    "4.1 API Architecture",
                    "4.2 Endpoint Documentation",
                    "4.3 Request/Response Formats",
                    "4.4 Authentication & Authorization",
                    "4.5 Error Handling"
                ]},
                {"id": "sec5_ui", "title": "5. UI/UX Design", "pages": 4, "type": "generate", "subsections": [
                    "5.1 User Interface Overview",
                    "5.2 Navigation Flow",
                    "5.3 Screen Wireframes",
                    "5.4 Design Guidelines"
                ]},
                {"id": "sec6_security", "title": "6. Security Design", "pages": 3, "type": "generate", "subsections": [
                    "6.1 Security Requirements",
                    "6.2 Authentication Mechanism",
                    "6.3 Data Protection",
                    "6.4 Security Best Practices"
                ]},
                {"id": "sec7_deployment", "title": "7. Deployment Architecture", "pages": 3, "type": "generate", "subsections": [
                    "7.1 Deployment Overview",
                    "7.2 Infrastructure Requirements",
                    "7.3 Scalability Considerations",
                    "7.4 Monitoring Strategy"
                ]},
            ]
        },
        DocumentType.PPT: {
            "name": "Project Presentation",
            "estimated_slides": 25,
            "sections": [
                {"id": "title", "title": "Title Slide", "slides": 1, "type": "template"},
                {"id": "agenda", "title": "Agenda", "slides": 1, "type": "generate"},
                {"id": "intro", "title": "Introduction", "slides": 3, "type": "generate"},
                {"id": "problem", "title": "Problem Statement", "slides": 2, "type": "generate"},
                {"id": "objectives", "title": "Objectives", "slides": 2, "type": "generate"},
                {"id": "literature", "title": "Literature Review", "slides": 2, "type": "generate"},
                {"id": "methodology", "title": "Methodology", "slides": 2, "type": "generate"},
                {"id": "architecture", "title": "System Architecture", "slides": 2, "type": "generate"},
                {"id": "implementation", "title": "Implementation", "slides": 3, "type": "generate"},
                {"id": "demo", "title": "Demo Screenshots", "slides": 3, "type": "generate"},
                {"id": "testing", "title": "Testing Results", "slides": 2, "type": "generate"},
                {"id": "conclusion", "title": "Conclusion", "slides": 1, "type": "generate"},
                {"id": "future", "title": "Future Scope", "slides": 1, "type": "generate"},
                {"id": "references", "title": "References", "slides": 1, "type": "generate"},
                {"id": "thankyou", "title": "Thank You", "slides": 1, "type": "template"},
            ]
        },
        DocumentType.VIVA_QA: {
            "name": "Viva Questions and Answers",
            "estimated_pages": 15,
            "sections": [
                {"id": "cover", "title": "Cover Page", "pages": 1, "type": "template"},
                {"id": "intro_qa", "title": "Project Introduction Questions", "pages": 2, "type": "generate", "subsections": [
                    "Q1: What is the project about?",
                    "Q2: Why did you choose this project?",
                    "Q3: What problem does it solve?",
                    "Q4: Who are the target users?",
                    "Q5: What are the main objectives?"
                ]},
                {"id": "tech_qa", "title": "Technology Stack Questions", "pages": 3, "type": "generate", "subsections": [
                    "Q1: What technologies did you use and why?",
                    "Q2: Explain the frontend technology choice",
                    "Q3: Explain the backend technology choice",
                    "Q4: Why did you choose this database?",
                    "Q5: What are the advantages of your tech stack?",
                    "Q6: What are the limitations of your tech stack?"
                ]},
                {"id": "architecture_qa", "title": "System Architecture Questions", "pages": 2, "type": "generate", "subsections": [
                    "Q1: Explain the system architecture",
                    "Q2: What design patterns did you use?",
                    "Q3: How does the frontend communicate with backend?",
                    "Q4: Explain the database schema design",
                    "Q5: How did you handle authentication?"
                ]},
                {"id": "implementation_qa", "title": "Implementation Questions", "pages": 3, "type": "generate", "subsections": [
                    "Q1: What were the main challenges in implementation?",
                    "Q2: How did you overcome those challenges?",
                    "Q3: Explain a critical code module",
                    "Q4: How did you manage state in the application?",
                    "Q5: Explain the API design",
                    "Q6: How did you handle error handling?"
                ]},
                {"id": "testing_qa", "title": "Testing Questions", "pages": 2, "type": "generate", "subsections": [
                    "Q1: What testing strategies did you use?",
                    "Q2: How did you perform unit testing?",
                    "Q3: Explain integration testing approach",
                    "Q4: How did you handle bug fixes?",
                    "Q5: What is the test coverage?"
                ]},
                {"id": "future_qa", "title": "Future Scope Questions", "pages": 1, "type": "generate", "subsections": [
                    "Q1: What are the limitations of your project?",
                    "Q2: What future enhancements do you suggest?",
                    "Q3: How would you scale this application?",
                    "Q4: What security improvements would you add?"
                ]},
                {"id": "general_qa", "title": "General Technical Questions", "pages": 1, "type": "generate", "subsections": [
                    "Q1: Explain the software development lifecycle you followed",
                    "Q2: How did you manage version control?",
                    "Q3: What tools did you use for development?",
                    "Q4: How did you collaborate as a team?"
                ]}
            ]
        }
    }

    OUTLINE_SYSTEM_PROMPT = """You are a Document Outline Generator for academic projects.

Your task is to create a detailed outline for a specific section of a document.

RULES:
1. Output ONLY valid JSON
2. Be specific to the project provided
3. Include realistic content points
4. Academic tone
5. No placeholders

Output format:
{
    "section_id": "chapter_id",
    "title": "Section Title",
    "subsections": [
        {
            "id": "1.1",
            "title": "Subsection Title",
            "key_points": ["point1", "point2", ...],
            "estimated_words": 500
        }
    ],
    "diagrams_needed": ["diagram_type1", ...],
    "tables_needed": ["table_type1", ...]
}
"""

    CONTENT_SYSTEM_PROMPT = """You are an Academic Content Writer for project documentation.

CRITICAL PAGE LIMIT RULES (TOTAL DOCUMENT MUST BE 60-80 PAGES MAX):
1. STRICTLY follow word limits - do NOT exceed them
2. Main section intro: 80-100 words MAXIMUM
3. Each subsection: 60-80 words MAXIMUM (keep it SHORT)
4. Be CONCISE - avoid repetition and filler content
5. Quality over quantity - each sentence must add value
6. NO placeholders or "TODO" - write real content
7. Use bullet points to save space where appropriate

WRITING STYLE:
- Academic but CONCISE
- Use project data provided
- Include technical terms in `code` format
- **bold** only for key emphasis

IMPORTANT: You MUST output ONLY valid JSON. No markdown, no explanations, no text before or after the JSON.

JSON FORMAT (output exactly this structure):
{
    "section_id": "id",
    "title": "Title",
    "content": "Brief intro (80-100 words MAX)...",
    "subsections": [
        {
            "id": "1.1",
            "title": "Subsection Title",
            "content": "Concise content (60-80 words MAX)..."
        }
    ],
    "tables": [
        {
            "id": "table_1",
            "caption": "Table caption",
            "headers": ["Column 1", "Column 2"],
            "rows": [["data1", "data2"]]
        }
    ]
}
"""

    VIVA_QA_SYSTEM_PROMPT = """You are a Viva Questions and Answers Generator for academic projects.

Your task is to generate concise Q&A content for viva preparation.

CRITICAL RULES:
1. Generate REALISTIC questions that examiners would ask
2. Provide SHORT, CONCISE answers (2-4 sentences only)
3. Each answer should be 30-50 words maximum
4. Be direct and to the point - no unnecessary elaboration
5. Cover key points without lengthy explanations
6. NO placeholders - provide real, usable answers

IMPORTANT: You MUST output ONLY valid JSON. No markdown, no explanations, no text before or after the JSON.

JSON FORMAT (output exactly this structure):
{
    "section_id": "id",
    "title": "Section Title",
    "content": "Brief intro (1 sentence)...",
    "qa_pairs": [
        {
            "question": "Question text?",
            "answer": "Short 2-4 sentence answer.",
            "follow_up_tips": "Brief tip (1 sentence)."
        }
    ]
}
"""

    def __init__(self, model: str = "sonnet"):
        super().__init__(
            name="Chunked Document Generator",
            role="chunked_document_generator",
            capabilities=[
                "large_document_generation",
                "section_by_section_generation",
                "word_document_creation",
                "ppt_creation",
                "60_80_page_documents"
            ],
            model=model  # Use sonnet for detailed document generation
        )

    async def process(self, context: AgentContext) -> Dict[str, Any]:
        """
        Process document generation request.

        Args:
            context: AgentContext with user request and metadata

        Returns:
            Dict with document generation result
        """
        # Extract document type from context
        doc_type_str = context.metadata.get("document_type", "project_report")
        try:
            document_type = DocumentType(doc_type_str)
        except ValueError:
            document_type = DocumentType.PROJECT_REPORT

        project_data = context.metadata.get("project_data", {
            "name": context.metadata.get("project_name", "Project"),
            "description": context.user_request
        })

        # Generate document and collect results
        results = []
        async for event in self.generate_document(context, document_type, project_data):
            results.append(event)

        # Return final result
        return {
            "success": True,
            "document_type": document_type.value,
            "events": results,
            "final_document": results[-1] if results else None
        }

    def _validate_and_enrich_project_data(self, project_data: Dict) -> Dict:
        """
        Validate and enrich project data with defaults for missing fields.
        Ensures document generation has all required information.
        """
        # Create a copy to avoid modifying the original
        data = dict(project_data)

        # Required fields with defaults
        if not data.get("project_name"):
            data["project_name"] = "Software Project"
            logger.warning("[ChunkedDoc] Missing project_name, using default")

        if not data.get("project_type"):
            data["project_type"] = "Software Application"

        # Ensure technologies is a dict
        if not isinstance(data.get("technologies"), dict):
            data["technologies"] = {}

        # Ensure features is a list
        if not isinstance(data.get("features"), list):
            data["features"] = []

        # If no technologies specified, try to infer from project type
        if not data["technologies"]:
            project_type_lower = data.get("project_type", "").lower()
            if "web" in project_type_lower or "react" in project_type_lower:
                data["technologies"] = {"frontend": ["React", "TypeScript"], "backend": ["Node.js"]}
            elif "java" in project_type_lower or "spring" in project_type_lower:
                data["technologies"] = {"backend": ["Java", "Spring Boot"], "database": ["PostgreSQL"]}
            elif "python" in project_type_lower or "django" in project_type_lower or "fastapi" in project_type_lower:
                data["technologies"] = {"backend": ["Python", "FastAPI"], "database": ["PostgreSQL"]}
            elif "mobile" in project_type_lower or "android" in project_type_lower:
                data["technologies"] = {"mobile": ["React Native"], "backend": ["Node.js"]}
            else:
                data["technologies"] = {"backend": ["Python"], "database": ["SQLite"]}
            logger.info(f"[ChunkedDoc] Inferred technologies: {data['technologies']}")

        # If no features specified, generate generic ones based on project type
        if not data["features"]:
            data["features"] = [
                "User Authentication",
                "Data Management",
                "Reporting Dashboard",
                "API Integration"
            ]
            logger.info(f"[ChunkedDoc] Using default features")

        # Ensure database_tables is a list
        if not isinstance(data.get("database_tables"), list):
            data["database_tables"] = []

        # Ensure api_endpoints is a list
        if not isinstance(data.get("api_endpoints"), list):
            data["api_endpoints"] = []

        # Ensure code_files is a list
        if not isinstance(data.get("code_files"), list):
            data["code_files"] = []

        return data

    async def generate_document(
        self,
        context: AgentContext,
        document_type: DocumentType,
        project_data: Dict,
        college_info: Optional[CollegeInfo] = None,
        progress_callback: Optional[callable] = None,
        parallel: bool = True,
        max_retries: int = 3
    ) -> AsyncGenerator[Dict, None]:
        """
        Generate document section by section with progress updates.

        Args:
            context: Agent context
            document_type: Type of document to generate
            project_data: Project data for content generation
            college_info: College information for Certificate, Declaration, Acknowledgement
            progress_callback: Optional callback for progress updates
            parallel: Enable parallel section generation (faster)
            max_retries: Maximum retries for failed sections

        Yields progress events and final document.
        """
        try:
            # Validate and enrich project data before starting
            project_data = self._validate_and_enrich_project_data(project_data)

            # Reset token tracking for this document generation
            self.reset_token_tracking()

            # For SRS/PPT documents, use domain-specific template based on project data
            if document_type == DocumentType.SRS:
                structure = _get_srs_structure(project_data)
                detected_domain = structure.get("detected_domain", "default")
                logger.info(f"[ChunkedDoc] SRS template selected: {structure['name']} (domain: {detected_domain})")
            elif document_type == DocumentType.PPT:
                structure = _get_ppt_structure(project_data)
                detected_domain = structure.get("detected_domain", "default")
                logger.info(f"[ChunkedDoc] PPT template selected: {structure['name']} (domain: {detected_domain})")
            else:
                structure = self.DOCUMENT_STRUCTURES.get(document_type)
            if not structure:
                raise ValueError(f"Unknown document type: {document_type}")

            total_sections = len(structure["sections"])
            generated_sections = []

            # Set project title in college_info if not set
            if college_info and not college_info.project_title:
                college_info.project_title = project_data.get("project_name", "Project")

            logger.info(f"[ChunkedDoc] Starting {document_type.value} generation with {total_sections} sections")
            logger.info(f"[ChunkedDoc] Parallel: {parallel}, Max Retries: {max_retries}")

            # Yield start event
            yield {
                "type": "start",
                "document_type": document_type.value,
                "total_sections": total_sections,
                "estimated_pages": structure.get("estimated_pages", structure.get("estimated_slides", 20)),
                "parallel_enabled": parallel
            }

            # Phase 1: Generate outline for the entire document
            yield {"type": "phase", "phase": "outline", "message": "Generating document outline..."}

            outline = await self._generate_document_outline(
                document_type,
                structure,
                project_data
            )

            yield {"type": "outline_complete", "outline": outline}

            # Phase 2: Generate each section
            yield {"type": "phase", "phase": "content", "message": "Generating section content..."}

            if parallel:
                # Parallel generation for AI sections
                generated_sections = await self._generate_sections_parallel(
                    structure["sections"],
                    outline,
                    project_data,
                    document_type,
                    college_info,
                    max_retries
                )
                yield {
                    "type": "sections_complete",
                    "sections_generated": len(generated_sections),
                    "progress": 100
                }
            else:
                # Sequential generation
                for idx, section in enumerate(structure["sections"]):
                    section_id = section["id"]
                    section_title = section["title"]
                    section_type = section["type"]

                    yield {
                        "type": "section_start",
                        "section_id": section_id,
                        "section_title": section_title,
                        "progress": (idx / total_sections) * 100
                    }

                    content = await self._generate_single_section(
                        section,
                        outline,
                        project_data,
                        document_type,
                        college_info,
                        max_retries
                    )

                    generated_sections.append({
                        "section_id": section_id,
                        "title": section_title,
                        "content": content,
                        "type": section_type
                    })

                    yield {
                        "type": "section_complete",
                        "section_id": section_id,
                        "section_title": section_title,
                        "progress": ((idx + 1) / total_sections) * 100
                    }

                    # Small delay to avoid rate limiting
                    await asyncio.sleep(0.5)

            # Phase 3: Generate UML Diagrams
            yield {"type": "phase", "phase": "diagrams", "message": "Generating UML diagrams..."}

            # Get project_id and user_id from context for isolation
            project_id = context.project_id if context else None
            user_id = context.user_id if context else None

            diagrams = await self._generate_all_diagrams(project_data, project_id=project_id, user_id=user_id)

            yield {
                "type": "diagrams_complete",
                "diagrams_generated": len(diagrams),
                "diagram_types": list(diagrams.keys())
            }

            # Store diagram paths in project_data for assembly
            project_data["generated_diagrams"] = diagrams

            # Phase 4: Assemble document
            yield {"type": "phase", "phase": "assembly", "message": "Assembling final document..."}

            if document_type == DocumentType.PPT:
                final_doc = await self._assemble_ppt(generated_sections, project_data, project_id, user_id)
            else:
                final_doc = await self._assemble_word_document(
                    generated_sections,
                    project_data,
                    document_type,
                    project_id,
                    user_id
                )

            # Get token usage for this document generation
            token_usage = self.get_token_usage()

            yield {
                "type": "complete",
                "document_type": document_type.value,
                "file_path": final_doc["path"],
                "pages": final_doc.get("pages", 0),
                "sections_generated": len(generated_sections),
                "token_usage": token_usage
            }

        except Exception as e:
            logger.error(f"[ChunkedDoc] Error: {e}", exc_info=True)
            yield {
                "type": "error",
                "error": str(e)
            }

    async def _generate_document_outline(
        self,
        document_type: DocumentType,
        structure: Dict,
        project_data: Dict
    ) -> Dict:
        """Generate outline for all sections"""

        prompt = f"""Generate a detailed outline for a {structure['name']}.

PROJECT: {project_data.get('project_name', 'Project')}
TYPE: {project_data.get('project_type', 'Software Project')}

TECHNOLOGIES:
{json.dumps(project_data.get('technologies', {}), indent=2)}

FEATURES:
{json.dumps(project_data.get('features', []), indent=2)}

SECTIONS TO OUTLINE:
{json.dumps([{"id": s["id"], "title": s["title"], "subsections": s.get("subsections", [])} for s in structure["sections"] if s["type"] == "generate"], indent=2)}

Generate outline with key points for each section.
Output JSON with section_id as keys.
"""

        response = await self._call_claude(
            system_prompt=self.OUTLINE_SYSTEM_PROMPT,
            user_prompt=prompt,
            temperature=0.3,
            max_tokens=4096
        )

        result = self._parse_json(response)
        if not result:
            logger.warning("[ChunkedDoc] Empty outline, creating default structure")
            result = {"sections": {}}
        return result

    async def _generate_section_content(
        self,
        section: Dict,
        outline: Dict,
        project_data: Dict,
        document_type: DocumentType
    ) -> Dict:
        """Generate content for a single section"""

        section_id = section["id"]
        section_title = section["title"]
        subsections = section.get("subsections", [])
        target_pages = section.get("pages", section.get("slides", 2))

        # Estimate words needed (250 words per page for docs, 50 per slide)
        if document_type == DocumentType.PPT:
            target_words = target_pages * 50
        else:
            target_words = target_pages * 250

        # Use different prompt and system prompt for VIVA_QA
        if document_type == DocumentType.VIVA_QA:
            prompt = f"""Generate comprehensive VIVA Questions and Answers for this section:

SECTION: {section_title}

QUESTIONS TO ANSWER:
{json.dumps(subsections, indent=2)}

PROJECT DATA:
- Name: {project_data.get('project_name')}
- Type: {project_data.get('project_type')}
- Technologies: {json.dumps(project_data.get('technologies', {}))}
- Features: {json.dumps(project_data.get('features', []))}
- API Endpoints: {json.dumps(project_data.get('api_endpoints', [])[:10])}
- Database Tables: {json.dumps(project_data.get('database_tables', []))}

REQUIREMENTS:
1. Generate realistic viva questions that examiners would ask
2. Provide detailed, technical answers (100-200 words each)
3. Include follow-up tips for each question
4. Use project-specific details in answers
5. NO placeholders - provide actual answers
6. Each Q&A should demonstrate deep understanding of the project

Generate Q&A content in JSON format with qa_pairs array.
"""
            system_prompt = self.VIVA_QA_SYSTEM_PROMPT
        else:
            prompt = f"""Generate DETAILED content for this section:

SECTION: {section_title}
TARGET WORDS: {target_words} minimum

SUBSECTIONS TO COVER:
{json.dumps(subsections, indent=2)}

OUTLINE POINTS:
{json.dumps(outline, indent=2)}

PROJECT DATA:
- Name: {project_data.get('project_name')}
- Type: {project_data.get('project_type')}
- Technologies: {json.dumps(project_data.get('technologies', {}))}
- Features: {json.dumps(project_data.get('features', []))}
- API Endpoints: {json.dumps(project_data.get('api_endpoints', [])[:10])}
- Database Tables: {json.dumps(project_data.get('database_tables', []))}

REQUIREMENTS:
1. Write detailed, professional content
2. Include specific technical details from the project
3. Academic writing style
4. Create relevant tables and figures
5. Minimum {target_words} words
6. NO placeholders

Generate complete section content in JSON format.
"""
            system_prompt = self.CONTENT_SYSTEM_PROMPT

        response = await self._call_claude(
            system_prompt=system_prompt,
            user_prompt=prompt,
            temperature=0.4,
            max_tokens=8192  # Sonnet supports up to 8192 output tokens
        )

        logger.info(f"[ChunkedDoc] Raw response length for {section_id}: {len(response) if response else 0} chars")

        result = self._parse_json(response, section_info=section)

        # Validate content was generated
        if not result.get("content") and not result.get("subsections") and not result.get("qa_pairs"):
            logger.warning(f"[ChunkedDoc] Empty content for section {section_id}, using fallback")
            result = self._create_fallback_content(section)

        return result

    def _get_template_content(self, section_id: str, project_data: Dict, college_info: Optional[CollegeInfo] = None) -> Dict:
        """Get pre-defined template content with college information"""

        # Use college_info if provided, otherwise use defaults from project_data
        if college_info:
            ci = college_info
        else:
            # Build student list from project_data
            students = []
            author = project_data.get("author")
            roll_number = project_data.get("roll_number")
            if author and author != "Student":
                students.append({
                    "name": author,
                    "roll_number": roll_number or ""
                })

            ci = CollegeInfo(
                college_name=project_data.get("institution", "University"),
                affiliated_to=project_data.get("university_name", ""),
                department=project_data.get("department", "Computer Science and Engineering"),
                guide_name=project_data.get("guide", "Project Guide"),
                hod_name=project_data.get("hod_name", ""),
                project_title=project_data.get("project_name", "Project"),
                students=students if students else [{"name": "Student Name", "roll_number": ""}]
            )

        # Format student list for display
        student_names = [s.get("name", "Student") for s in ci.students]
        student_list_formatted = "\n".join([
            f"{i+1}. {s.get('name', 'Student')} ({s.get('roll_number', '')})"
            for i, s in enumerate(ci.students)
        ])

        templates = {
            "cover": {
                "type": "cover_page",
                "project_name": ci.project_title,
                "subtitle": project_data.get("project_type", "Software Project"),
                "college_name": ci.college_name,
                "affiliated_to": ci.affiliated_to,
                "college_address": ci.college_address,
                "department": ci.department,
                "academic_year": ci.academic_year,
                "students": ci.students,
                "guide_name": ci.guide_name,
                "date": ci.date
            },
            "certificate": {
                "type": "certificate",
                "college_name": ci.college_name,
                "affiliated_to": ci.affiliated_to,
                "college_address": ci.college_address,
                "department": ci.department,
                "project_title": ci.project_title,
                "academic_year": ci.academic_year,
                "students": ci.students,
                "guide_name": ci.guide_name,
                "hod_name": ci.hod_name,
                "principal_name": ci.principal_name,
                "date": ci.date,
                "content": f"""
This is to certify that the project entitled "{ci.project_title}" is a bonafide work
carried out by the following students:

{student_list_formatted}

in partial fulfillment of the requirements for the award of Bachelor of Technology
in Computer Science and Engineering from {ci.college_name} during the academic year {ci.academic_year}.

This project work has been approved as it satisfies the academic requirements prescribed
for the said degree.


Project Guide                    Head of Department                    Principal
{ci.guide_name}                  {ci.hod_name}                         {ci.principal_name}

Signature: ____________          Signature: ____________               Signature: ____________
Date: ____________               Date: ____________                    Date: ____________


External Examiner
Name: ________________________
Signature: ____________________
Date: ________________________
"""
            },
            "declaration": {
                "type": "declaration",
                "college_name": ci.college_name,
                "department": ci.department,
                "project_title": ci.project_title,
                "guide_name": ci.guide_name,
                "students": ci.students,
                "date": ci.date,
                "content": f"""
DECLARATION

We, the undersigned, hereby declare that the project entitled "{ci.project_title}"
submitted to {ci.college_name}, {ci.department}, is a record of an original work done
by us under the guidance of {ci.guide_name}.

This project work is submitted in partial fulfillment of the requirements for the award
of the degree of Bachelor of Technology in Computer Science and Engineering.

We further declare that:

1. This project is based on our original work.
2. This project has not been submitted previously for any degree or examination in any other university.
3. All sources of information have been duly acknowledged.
4. We have followed the guidelines provided by the institute for preparing this report.


Student Signatures:

{chr(10).join([f"{s.get('name', 'Student'):30} {s.get('roll_number', ''):15} ________________" for s in ci.students])}


Date: {ci.date}
Place: {ci.college_name}
"""
            },
            "acknowledgement": {
                "type": "acknowledgement",
                "guide_name": ci.guide_name,
                "hod_name": ci.hod_name,
                "principal_name": ci.principal_name,
                "college_name": ci.college_name,
                "department": ci.department,
                "students": ci.students,
                "date": ci.date,
                "content": f"""
ACKNOWLEDGEMENT

We take this opportunity to express our profound gratitude and deep regards to our
project guide {ci.guide_name} for the exemplary guidance, monitoring, and constant
encouragement throughout the course of this project.

We would like to express our sincere thanks to {ci.hod_name}, Head of Department,
{ci.department}, for providing us with the opportunity to work on this project.

We also express our sincere gratitude to {ci.principal_name}, Principal, {ci.college_name},
for providing us with the necessary facilities and support.

We extend our heartfelt thanks to all the faculty members of the {ci.department}
for their valuable suggestions and support during the development of this project.

We would also like to thank our family and friends for their constant support and encouragement.

Finally, we thank all those who directly or indirectly helped us in the successful
completion of this project.


Team Members:
{chr(10).join([f"{i+1}. {s.get('name', 'Student')}" for i, s in enumerate(ci.students)])}


Date: {ci.date}
Place: {ci.college_name}
"""
            },
            "title": {
                "type": "title_slide",
                "project_name": ci.project_title,
                "subtitle": project_data.get("project_type", "Software Project"),
                "college_name": ci.college_name,
                "department": ci.department,
                "presented_by": student_names,
                "guide_name": ci.guide_name,
                "academic_year": ci.academic_year
            },
            "thankyou": {
                "type": "thankyou_slide",
                "message": "Thank You!",
                "college_name": ci.college_name,
                "presented_by": student_names,
                "contact": project_data.get("email", "")
            }
        }

        return templates.get(section_id, {"type": "template", "content": ""})

    def _get_code_content(self, section_id: str, project_data: Dict) -> Dict:
        """Get code snippets from project"""

        code_files = project_data.get("code_files", [])

        # Select representative files
        selected_code = []
        for file in code_files[:5]:  # Limit to 5 files
            selected_code.append({
                "filename": file.get("path", ""),
                "language": file.get("language", ""),
                "content": file.get("content", "")[:2000]  # Truncate long files
            })

        return {
            "type": "code_appendix",
            "files": selected_code
        }

    async def _assemble_word_document(
        self,
        sections: List[Dict],
        project_data: Dict,
        document_type: DocumentType,
        project_id: str = None,
        user_id: str = None
    ) -> Dict:
        """Assemble sections into Word document using python-docx"""

        from app.modules.automation.word_generator import WordDocumentGenerator

        generator = WordDocumentGenerator()

        file_path = await generator.create_document(
            sections=sections,
            project_data=project_data,
            document_type=document_type.value,
            project_id=project_id,
            user_id=user_id
        )

        return {
            "path": file_path,
            "format": "docx",
            "pages": sum(s.get("pages", 2) for s in sections if isinstance(s, dict))
        }

    async def _assemble_ppt(
        self,
        sections: List[Dict],
        project_data: Dict,
        project_id: str = None,
        user_id: str = None
    ) -> Dict:
        """Assemble sections into PowerPoint"""

        from app.modules.automation.ppt_generator_v2 import PPTGeneratorV2

        generator = PPTGeneratorV2()

        file_path = await generator.create_presentation(
            sections=sections,
            project_data=project_data,
            project_id=project_id,
            user_id=user_id
        )

        return {
            "path": file_path,
            "format": "pptx",
            "slides": sum(s.get("slides", 1) for s in sections if isinstance(s, dict))
        }

    def _parse_json(self, response: str, section_info: Optional[Dict] = None) -> Dict:
        """
        Parse JSON from Claude response with multiple fallback strategies.

        Args:
            response: The raw response from Claude
            section_info: Optional section metadata for fallback content generation

        Returns:
            Parsed JSON dict, or fallback content if parsing fails
        """
        import re

        if not response or not response.strip():
            logger.warning("[ChunkedDoc] Empty response received")
            return self._create_fallback_content(section_info)

        # Strategy 1: Try direct JSON parsing
        try:
            result = json.loads(response.strip())
            if isinstance(result, dict):
                return result
        except json.JSONDecodeError:
            pass

        # Strategy 2: Find JSON between first { and last }
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                result = json.loads(json_str)
                if isinstance(result, dict):
                    return result
        except json.JSONDecodeError:
            pass

        # Strategy 3: Try to find JSON in code blocks (various formats)
        try:
            # Try multiple code block patterns
            patterns = [
                r'```(?:json)?\s*(\{[\s\S]*?\})\s*```',
                r'```\s*(\{[\s\S]*?\})\s*```',
                r'`(\{[^`]+\})`',
            ]
            for pattern in patterns:
                json_match = re.search(pattern, response)
                if json_match:
                    result = json.loads(json_match.group(1))
                    if isinstance(result, dict):
                        return result
        except (json.JSONDecodeError, AttributeError):
            pass

        # Strategy 4: Fix common JSON issues and retry
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                # Fix common issues
                json_str = re.sub(r',\s*}', '}', json_str)  # Remove trailing commas before }
                json_str = re.sub(r',\s*]', ']', json_str)  # Remove trailing commas before ]
                json_str = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)  # Remove control chars
                json_str = re.sub(r'"\s*\n\s*"', '" "', json_str)  # Fix split strings
                json_str = re.sub(r'\\(?!["\\/bfnrt])', r'\\\\', json_str)  # Escape unescaped backslashes
                result = json.loads(json_str)
                if isinstance(result, dict):
                    return result
        except json.JSONDecodeError:
            pass

        # Strategy 5: Try parsing with relaxed JSON (handle single quotes, unquoted keys)
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                # Replace single quotes with double quotes (careful with apostrophes)
                json_str = re.sub(r"(?<=[{,\s])'([^']+)'(?=\s*:)", r'"\1"', json_str)  # Keys
                json_str = re.sub(r":\s*'([^']*)'(?=[,}\]])", r': "\1"', json_str)  # Values
                result = json.loads(json_str)
                if isinstance(result, dict):
                    return result
        except json.JSONDecodeError:
            pass

        # Strategy 6: Extract content directly from text as fallback
        logger.warning("[ChunkedDoc] JSON parsing failed after all strategies, extracting content from raw text")
        return self._extract_content_from_text(response, section_info)

    def _create_fallback_content(self, section_info: Optional[Dict] = None, project_data: Optional[Dict] = None) -> Dict:
        """Create fallback content structure when parsing fails completely."""
        section_id = section_info.get("id", "section") if section_info else "section"
        title = section_info.get("title", "Section") if section_info else "Section"
        subsections = section_info.get("subsections", []) if section_info else []

        # Get project-specific data for more relevant fallback content
        project_name = "the project"
        technologies = ""
        features = ""
        if project_data:
            project_name = project_data.get("project_name", "the project")
            tech_dict = project_data.get("technologies", {})
            if tech_dict:
                tech_list = []
                for category, items in tech_dict.items():
                    if isinstance(items, list):
                        tech_list.extend(items[:2])
                    elif isinstance(items, str):
                        tech_list.append(items)
                technologies = ", ".join(tech_list[:5])
            features_list = project_data.get("features", [])
            if features_list:
                features = ", ".join(features_list[:3])

        # Generate more contextual fallback content based on section type
        content_map = {
            "introduction": f"This chapter introduces {project_name}, providing an overview of the system's purpose, scope, and objectives. The project aims to deliver a comprehensive solution using modern technologies{f' including {technologies}' if technologies else ''}.",
            "abstract": f"This document presents {project_name}, a software system designed to address specific user needs{f' through features like {features}' if features else ''}. The system is built using industry-standard practices and modern technologies.",
            "literature_survey": f"This section reviews existing solutions and research related to {project_name}. The survey covers current industry practices, similar systems, and identifies gaps that this project addresses.",
            "system_requirements": f"This chapter details the software and hardware requirements for {project_name}{f', including the technology stack: {technologies}' if technologies else ''}. Both functional and non-functional requirements are specified.",
            "system_design": f"The system design chapter presents the architecture of {project_name}, including high-level design, module descriptions, and data flow diagrams. The design follows best practices for maintainability and scalability.",
            "implementation": f"This chapter covers the implementation details of {project_name}{f' using {technologies}' if technologies else ''}. Code structure, key algorithms, and development methodology are discussed.",
            "testing": f"This section describes the testing strategy for {project_name}, including unit testing, integration testing, and user acceptance testing. Test cases and results are documented.",
            "conclusion": f"This chapter summarizes the achievements of {project_name}{f', which successfully implements {features}' if features else ''}. Future enhancements and lessons learned are also discussed.",
            "references": "This section lists all references, including academic papers, technical documentation, and online resources used during the development of the project.",
        }

        # Try to match section id to get relevant content
        base_content = content_map.get(section_id, "")
        if not base_content:
            # Try partial matching
            for key, value in content_map.items():
                if key in section_id.lower() or section_id.lower() in key:
                    base_content = value
                    break

        if not base_content:
            base_content = f"This section covers {title} for {project_name}. The content provides a comprehensive overview of the relevant topics and concepts."

        # Generate subsections if we have them defined
        generated_subsections = []
        for i, subsection in enumerate(subsections[:5]):
            sub_title = subsection if isinstance(subsection, str) else subsection.get("title", f"Subsection {i+1}")
            generated_subsections.append({
                "id": f"{section_id}.{i+1}",
                "title": sub_title,
                "content": f"This subsection addresses {sub_title} in the context of {project_name}."
            })

        return {
            "section_id": section_id,
            "title": title,
            "content": base_content,
            "subsections": generated_subsections,
            "fallback": True,
            "fallback_reason": "JSON parsing failed - using project-specific template"
        }

    def _extract_content_from_text(self, response: str, section_info: Optional[Dict] = None) -> Dict:
        """
        Extract meaningful content from non-JSON response.

        This attempts to salvage useful content even when JSON parsing fails.
        """
        import re

        section_id = section_info.get("id", "section") if section_info else "section"
        title = section_info.get("title", "Section") if section_info else "Section"

        # Clean up the response
        text = response.strip()

        # Remove JSON artifacts and code blocks
        text = re.sub(r'```[\s\S]*?```', '', text)
        text = re.sub(r'^\s*\{[\s\S]*', '', text)  # Remove JSON at start
        text = re.sub(r'[\s\S]*\}\s*$', '', text)  # Remove JSON at end

        # If we have useful text content, use it
        if len(text.strip()) > 100:
            # Split into paragraphs
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]

            if paragraphs:
                main_content = paragraphs[0] if len(paragraphs[0]) > 50 else ' '.join(paragraphs[:2])

                subsections = []
                for i, para in enumerate(paragraphs[1:5], 1):  # Take up to 4 more paragraphs as subsections
                    if len(para) > 50:
                        subsections.append({
                            "id": f"{section_id}.{i}",
                            "title": f"Section {section_id}.{i}",
                            "content": para
                        })

                return {
                    "section_id": section_id,
                    "title": title,
                    "content": main_content,
                    "subsections": subsections,
                    "extracted_from_text": True
                }

        # Ultimate fallback
        return self._create_fallback_content(section_info)

    async def _generate_sections_parallel(
        self,
        sections: List[Dict],
        outline: Dict,
        project_data: Dict,
        document_type: DocumentType,
        college_info: Optional[CollegeInfo],
        max_retries: int
    ) -> List[Dict]:
        """
        Generate multiple sections in parallel for faster document generation.

        Sections are grouped by type:
        - Template sections: Generated immediately (no API call)
        - AI sections: Generated in parallel batches of 3
        """
        generated_sections = []

        # Separate sections by type
        template_sections = []
        ai_sections = []
        auto_sections = []
        code_sections = []

        for section in sections:
            section_type = section.get("type", "generate")
            if section_type == "template":
                template_sections.append(section)
            elif section_type == "auto":
                auto_sections.append(section)
            elif section_type == "code":
                code_sections.append(section)
            else:
                ai_sections.append(section)

        logger.info(f"[ChunkedDoc] Parallel: {len(template_sections)} templates, {len(ai_sections)} AI sections")

        # 1. Generate template sections immediately (no API call)
        for section in template_sections:
            content = self._get_template_content(section["id"], project_data, college_info)
            generated_sections.append({
                "section_id": section["id"],
                "title": section["title"],
                "content": content,
                "type": "template"
            })

        # 2. Generate auto sections
        for section in auto_sections:
            generated_sections.append({
                "section_id": section["id"],
                "title": section["title"],
                "content": {"type": "auto", "section_id": section["id"]},
                "type": "auto"
            })

        # 3. Generate code sections
        for section in code_sections:
            content = self._get_code_content(section["id"], project_data)
            generated_sections.append({
                "section_id": section["id"],
                "title": section["title"],
                "content": content,
                "type": "code"
            })

        # 4. Generate AI sections in parallel batches
        BATCH_SIZE = 3  # Process 3 sections at a time to avoid rate limits

        for i in range(0, len(ai_sections), BATCH_SIZE):
            batch = ai_sections[i:i + BATCH_SIZE]

            # Create tasks for parallel execution
            tasks = [
                self._generate_single_section_with_retry(
                    section,
                    outline,
                    project_data,
                    document_type,
                    college_info,
                    max_retries
                )
                for section in batch
            ]

            # Execute batch in parallel
            results = await asyncio.gather(*tasks, return_exceptions=True)

            for section, result in zip(batch, results):
                if isinstance(result, Exception):
                    logger.error(f"[ChunkedDoc] Section {section['id']} failed with exception: {result}")
                    # Create project-specific fallback content instead of generic error
                    content = self._create_fallback_content(section, project_data)
                    content["error"] = str(result)
                elif result is None or (isinstance(result, dict) and not result.get("content") and not result.get("subsections")):
                    logger.warning(f"[ChunkedDoc] Section {section['id']} returned empty content")
                    content = self._create_fallback_content(section, project_data)
                    content["error"] = "Empty content returned"
                else:
                    content = result

                generated_sections.append({
                    "section_id": section["id"],
                    "title": section["title"],
                    "content": content,
                    "type": "generate"
                })

            # Small delay between batches to avoid rate limiting
            if i + BATCH_SIZE < len(ai_sections):
                await asyncio.sleep(1.0)

        # Sort sections by original order
        section_order = {s["id"]: idx for idx, s in enumerate(sections)}
        generated_sections.sort(key=lambda x: section_order.get(x["section_id"], 999))

        return generated_sections

    async def _generate_single_section(
        self,
        section: Dict,
        outline: Dict,
        project_data: Dict,
        document_type: DocumentType,
        college_info: Optional[CollegeInfo],
        max_retries: int
    ) -> Dict:
        """Generate a single section with appropriate method based on type."""
        section_type = section.get("type", "generate")

        if section_type == "template":
            return self._get_template_content(section["id"], project_data, college_info)
        elif section_type == "auto":
            return {"type": "auto", "section_id": section["id"]}
        elif section_type == "code":
            return self._get_code_content(section["id"], project_data)
        else:
            return await self._generate_single_section_with_retry(
                section, outline, project_data, document_type, college_info, max_retries
            )

    async def _generate_single_section_with_retry(
        self,
        section: Dict,
        outline: Dict,
        project_data: Dict,
        document_type: DocumentType,
        college_info: Optional[CollegeInfo],
        max_retries: int
    ) -> Dict:
        """
        Generate a single section with retry logic.

        Retries with exponential backoff on failure.
        Returns project-specific fallback content if all retries fail.
        """
        last_error = None
        last_content = None

        for attempt in range(max_retries):
            try:
                content = await self._generate_section_content(
                    section,
                    outline.get(section["id"], {}),
                    project_data,
                    document_type
                )

                # Check if content is valid (not empty, not error)
                if content:
                    # If it's a fallback but has actual content, accept it
                    if content.get("fallback") and content.get("content"):
                        logger.info(f"[ChunkedDoc] Section {section['id']} using fallback content (attempt {attempt + 1})")
                        return content
                    # If it has real content, accept it
                    if content.get("content") or content.get("subsections") or content.get("qa_pairs"):
                        if not content.get("error"):
                            return content
                        else:
                            last_content = content

            except Exception as e:
                last_error = e
                error_str = str(e).lower()

                # Check for rate limiting - use longer backoff
                if "rate" in error_str or "429" in error_str or "overload" in error_str:
                    wait_time = (3 ** attempt) + 2  # Longer backoff for rate limits: 3s, 5s, 11s, 29s
                    logger.warning(
                        f"[ChunkedDoc] Section {section['id']} rate limited (attempt {attempt + 1}/{max_retries})"
                    )
                else:
                    wait_time = (2 ** attempt) + 0.5  # Standard backoff: 1.5s, 2.5s, 4.5s
                    logger.warning(
                        f"[ChunkedDoc] Section {section['id']} attempt {attempt + 1}/{max_retries} failed: {e}"
                    )

                if attempt < max_retries - 1:
                    logger.info(f"[ChunkedDoc] Retrying in {wait_time:.1f}s...")
                    await asyncio.sleep(wait_time)

        # All retries failed - return project-specific fallback content
        logger.error(f"[ChunkedDoc] Section {section['id']} failed after {max_retries} attempts, using fallback")

        # If we got partial content from last attempt, try to use it
        if last_content and last_content.get("content"):
            last_content["fallback"] = True
            last_content["fallback_reason"] = f"Partial content after {max_retries} attempts"
            return last_content

        # Generate project-specific fallback content
        fallback = self._create_fallback_content(section, project_data)
        fallback["error"] = str(last_error) if last_error else "Generation failed after all retries"
        return fallback

    async def _generate_all_diagrams(self, project_data: Dict, project_id: str = None, user_id: str = None) -> Dict[str, str]:
        """
        Generate all UML diagrams for the document using DYNAMIC project data.
        Saves diagrams to S3 and PostgreSQL for persistence.

        Args:
            project_data: Project data for diagram generation
            project_id: Project ID for isolation
            user_id: User ID for isolation

        Returns:
            Dict mapping diagram type to file path (local paths for document assembly)
        """
        project_name = project_data.get('project_name', 'System')
        logger.info(f"[ChunkedDoc] Generating DYNAMIC UML diagrams for {project_name} (project_id={project_id}, user_id={user_id})")
        logger.info(f"[ChunkedDoc] Project features: {project_data.get('features', [])[:5]}")
        logger.info(f"[ChunkedDoc] Project tables: {project_data.get('database_tables', [])[:5]}")
        logger.info(f"[ChunkedDoc] Project technologies: {project_data.get('technologies', {})}")

        diagrams = {}

        try:
            # Use the new method that saves to S3 + PostgreSQL
            if project_id and user_id:
                try:
                    # Generate and save to cloud storage
                    results = await uml_generator.generate_all_diagrams_and_save(
                        project_data=project_data,
                        project_id=project_id,
                        user_id=user_id
                    )

                    # Extract local paths for document assembly (Word/PPT needs local files)
                    for diagram_type, result in results.items():
                        local_path = result.get('local_path')
                        if local_path and not local_path.startswith('['):
                            diagrams[diagram_type] = local_path
                            if result.get('saved_to_cloud'):
                                logger.info(f"[ChunkedDoc] {diagram_type} saved to S3: {result.get('s3_key')}")

                    logger.info(f"[ChunkedDoc] Generated {len(diagrams)} DYNAMIC UML diagrams (saved to S3+DB)")
                except Exception as cloud_err:
                    logger.warning(f"[ChunkedDoc] Cloud save failed, falling back to local: {cloud_err}")
                    # Fallback to local generation if cloud save fails
                    diagrams = uml_generator.generate_all_diagrams(
                        project_data=project_data,
                        project_id=project_id,
                        user_id=user_id
                    )
                    logger.info(f"[ChunkedDoc] Generated {len(diagrams)} UML diagrams (local fallback)")
            else:
                # Local-only generation if no user_id/project_id
                logger.warning("[ChunkedDoc] No project_id/user_id - saving diagrams locally only")
                diagrams = uml_generator.generate_all_diagrams(
                    project_data=project_data,
                    project_id=project_id,
                    user_id=user_id
                )
                logger.info(f"[ChunkedDoc] Generated {len(diagrams)} DYNAMIC UML diagrams (local only)")

        except Exception as e:
            logger.error(f"[ChunkedDoc] Error generating diagrams: {e}", exc_info=True)
            # Return empty dict but don't fail the whole document generation

        # Ensure we have at least the essential diagrams even if generation failed
        if not diagrams:
            logger.warning("[ChunkedDoc] No diagrams generated - document will proceed without diagrams")

        return diagrams

    def _extract_classes_for_diagram(self, project_data: Dict) -> List[Dict]:
        """Extract class information for class diagram"""
        classes = []

        tables = project_data.get('database_tables', [])
        for table in tables[:5]:
            classes.append({
                'name': table.title() if isinstance(table, str) else table.get('name', 'Entity'),
                'attributes': ['id', 'name', 'created_at', 'updated_at'],
                'methods': ['create', 'read', 'update', 'delete'],
                'relationships': []
            })

        if not classes:
            classes = [
                {
                    'name': 'User',
                    'attributes': ['id', 'name', 'email', 'password'],
                    'methods': ['login', 'logout', 'register'],
                    'relationships': []
                },
                {
                    'name': 'Controller',
                    'attributes': ['routes', 'middleware'],
                    'methods': ['handleRequest', 'validateInput'],
                    'relationships': [{'target': 'Service', 'type': 'association'}]
                },
                {
                    'name': 'Service',
                    'attributes': ['repository'],
                    'methods': ['processData', 'validateBusiness'],
                    'relationships': [{'target': 'Repository', 'type': 'association'}]
                },
                {
                    'name': 'Repository',
                    'attributes': ['database'],
                    'methods': ['find', 'save', 'delete'],
                    'relationships': []
                }
            ]

        return classes

    def _extract_entities_for_diagram(self, project_data: Dict) -> List[Dict]:
        """Extract entity information for ER diagram"""
        entities = []

        tables = project_data.get('database_tables', [])
        for table in tables[:6]:
            name = table if isinstance(table, str) else table.get('name', 'Entity')
            entities.append({
                'name': name,
                'attributes': ['id', 'name', 'description', 'created_at', 'updated_at'],
                'primary_key': 'id'
            })

        if not entities:
            entities = [
                {'name': 'User', 'attributes': ['id', 'name', 'email', 'password_hash'], 'primary_key': 'id'},
                {'name': 'Project', 'attributes': ['id', 'title', 'description', 'user_id'], 'primary_key': 'id'},
                {'name': 'Document', 'attributes': ['id', 'name', 'content', 'project_id'], 'primary_key': 'id'},
            ]

        return entities


    # =====================================================
    # STREAMING METHOD FOR RESUME/DOCUMENT ENDPOINTS
    # =====================================================

    async def generate_document_streaming(
        self,
        project_id: str,
        doc_type: DocumentType,
        db: "AsyncSession"
    ) -> AsyncGenerator[Dict, None]:
        """
        Generate a document by fetching project data from the database.

        This method is used by resume.py and other endpoints that need to generate
        documents without having the project data in memory.

        Args:
            project_id: Project ID to generate document for
            doc_type: Type of document to generate
            db: AsyncSession for database queries

        Yields:
            Progress events during document generation
        """
        import re
        from sqlalchemy import select
        from app.models.project import Project
        from app.models.project_file import ProjectFile
        from app.models.user import User
        from app.modules.agents.base_agent import AgentContext

        # Fetch project from database
        project_result = await db.execute(
            select(Project).where(Project.id == project_id)
        )
        project = project_result.scalar_one_or_none()

        if not project:
            yield {"type": "error", "data": {"error": "Project not found"}}
            return

        # Fetch project files with content
        files_result = await db.execute(
            select(ProjectFile).where(
                ProjectFile.project_id == project_id,
                ProjectFile.is_folder == False
            )
        )
        files = files_result.scalars().all()

        # Build files list with content
        files_created = []
        for f in files:
            files_created.append({
                "path": f.path,
                "content": f.content or ""
            })

        # Extract API endpoints from file content
        api_endpoints = self._extract_api_endpoints(files_created)

        # Extract database tables from file content
        database_tables = self._extract_database_tables(files_created)

        # Build project data
        project_data = {
            "project_name": project.title or "Project",
            "project_type": project.domain or "web_application",
            "description": project.prompt or "",
            "technologies": project.tech_stack or {},
            "features": project.requirements or [],
            "api_endpoints": api_endpoints,
            "database_tables": database_tables,
            "code_files": files_created
        }

        # Fetch user details for college_info
        user_result = await db.execute(
            select(User).where(User.id == project.user_id)
        )
        user = user_result.scalar_one_or_none()

        college_info = None
        if user:
            college_info = CollegeInfo(
                project_title=project.title or "Project",
                college_name=user.college_name or "College Name",
                affiliated_to=user.university_name or "Autonomous Institution",
                department=user.department or "Department of Computer Science and Engineering",
                academic_year=user.batch or "2024-2025",
                guide_name=user.guide_name or "Dr. Guide Name",
                hod_name=user.hod_name or "Dr. HOD Name",
                students=[{
                    "name": user.full_name or "Student Name",
                    "roll_number": user.roll_number or "ROLL001"
                }]
            )

        # Create agent context
        context = AgentContext(
            project_id=project_id,
            user_id=str(project.user_id) if project.user_id else None,
            user_request=f"Generate {doc_type.value} for {project.title}"
        )

        # Generate document using existing method
        async for event in self.generate_document(
            context=context,
            document_type=doc_type,
            project_data=project_data,
            college_info=college_info,
            parallel=True
        ):
            yield event

    def _extract_api_endpoints(self, files_created: list) -> list:
        """Extract API endpoints from generated code files."""
        import re
        endpoints = []

        patterns = [
            # FastAPI/Flask patterns
            r'@(?:app|router|api)\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
            # Express.js patterns
            r'(?:app|router)\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
            # Spring Boot patterns
            r'@(?:Get|Post|Put|Delete|Patch)Mapping\s*\(\s*(?:value\s*=\s*)?["\']([^"\']+)["\']',
            # Django patterns
            r'path\s*\(\s*["\']([^"\']+)["\']',
        ]

        for file_info in files_created:
            if isinstance(file_info, dict):
                file_path = file_info.get('path', '')
                content = file_info.get('content', '')
            else:
                continue

            if not any(ext in file_path for ext in ['.py', '.js', '.ts', '.java', '.go']):
                continue

            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if len(match) == 2:
                        method, path = match
                        endpoints.append({
                            "method": method.upper(),
                            "path": path,
                            "file": file_path
                        })
                    elif len(match) == 1:
                        endpoints.append({
                            "method": "GET",
                            "path": match[0],
                            "file": file_path
                        })

        return endpoints[:20]

    def _extract_database_tables(self, files_created: list) -> list:
        """
        Extract database tables with columns and relationships from generated code files.
        This is the key method for generating DYNAMIC UML diagrams.
        """
        import re
        import ast
        tables = []

        for file_info in files_created:
            if isinstance(file_info, dict):
                file_path = file_info.get('path', '')
                content = file_info.get('content', '')
            else:
                continue

            if not content:
                continue

            # Extract based on file type
            if file_path.endswith('.py'):
                tables.extend(self._extract_python_models(content, file_path))
            elif file_path.endswith('.prisma'):
                tables.extend(self._extract_prisma_models(content, file_path))
            elif file_path.endswith('.ts') or file_path.endswith('.js'):
                tables.extend(self._extract_typescript_models(content, file_path))
            elif file_path.endswith('.java'):
                tables.extend(self._extract_java_models(content, file_path))
            elif file_path.endswith('.sql'):
                tables.extend(self._extract_sql_tables(content, file_path))

        # Remove duplicates by name
        seen = set()
        unique_tables = []
        for table in tables:
            name = table.get('name', '')
            if name and name not in seen:
                seen.add(name)
                unique_tables.append(table)

        return unique_tables[:15]

    def _extract_python_models(self, content: str, file_path: str) -> list:
        """Extract SQLAlchemy/Django models with columns and relationships."""
        import re
        tables = []

        # Check if this looks like a model file
        if 'Column' not in content and 'models.Model' not in content and 'mapped_column' not in content:
            return tables

        # Find class definitions that inherit from Base/Model
        class_pattern = r'class\s+(\w+)\s*\([^)]*(?:Base|Model|DeclarativeBase)[^)]*\)\s*:'
        class_matches = list(re.finditer(class_pattern, content))

        for i, match in enumerate(class_matches):
            class_name = match.group(1)
            if class_name.lower() in ['base', 'model', 'basemodel']:
                continue

            # Get the class body (until next class or end of file)
            start_pos = match.end()
            if i + 1 < len(class_matches):
                end_pos = class_matches[i + 1].start()
            else:
                end_pos = len(content)

            class_body = content[start_pos:end_pos]

            # Extract columns
            columns = []
            relationships = []

            # Pattern for Column() or mapped_column()
            col_patterns = [
                # Standard: name = Column(Type, ...)
                r'(\w+)\s*=\s*Column\s*\(\s*(\w+)',
                # Mapped: name: Mapped[Type] = mapped_column(...)
                r'(\w+)\s*:\s*Mapped\s*\[\s*(\w+)\s*\]',
                # Annotated: name = mapped_column(Type, ...)
                r'(\w+)\s*=\s*mapped_column\s*\(\s*(\w+)',
            ]

            for col_pattern in col_patterns:
                for col_match in re.finditer(col_pattern, class_body):
                    col_name = col_match.group(1)
                    col_type = col_match.group(2)

                    # Skip special attributes
                    if col_name.startswith('__') or col_name in ['metadata', 'registry']:
                        continue

                    # Check if it's a primary key
                    is_pk = 'primary_key' in class_body[col_match.start():col_match.start()+200].lower()

                    # Check for ForeignKey
                    fk_match = re.search(r'ForeignKey\s*\(\s*["\'](\w+)\.(\w+)["\']',
                                        class_body[col_match.start():col_match.start()+300])
                    fk_ref = None
                    if fk_match:
                        fk_ref = fk_match.group(1)

                    columns.append({
                        'name': col_name,
                        'type': col_type,
                        'primary_key': is_pk or col_name == 'id',
                        'nullable': 'nullable=False' not in class_body[col_match.start():col_match.start()+200],
                        'foreign_key': fk_ref
                    })

                    # Add relationship if FK found
                    if fk_ref:
                        relationships.append({
                            'column': col_name,
                            'references': self._snake_to_pascal(fk_ref),
                            'type': 'many_to_one'
                        })

            # Find relationship() calls
            rel_pattern = r'(\w+)\s*=\s*relationship\s*\(\s*["\'](\w+)["\']'
            for rel_match in re.finditer(rel_pattern, class_body):
                rel_name = rel_match.group(1)
                rel_target = rel_match.group(2)
                # Check if it's one-to-many or many-to-one
                is_list = 'uselist=False' not in class_body[rel_match.start():rel_match.start()+200]
                relationships.append({
                    'column': rel_name,
                    'references': rel_target,
                    'type': 'one_to_many' if is_list else 'one_to_one'
                })

            if columns:
                tables.append({
                    'name': class_name,
                    'columns': columns,
                    'primary_key': next((c['name'] for c in columns if c.get('primary_key')), 'id'),
                    'relationships': relationships,
                    'file': file_path,
                    'type': 'sqlalchemy'
                })

        return tables

    def _extract_prisma_models(self, content: str, file_path: str) -> list:
        """Extract Prisma schema models with columns and relationships."""
        import re
        tables = []

        # Find model definitions
        model_pattern = r'model\s+(\w+)\s*\{([^}]+)\}'
        for match in re.finditer(model_pattern, content, re.DOTALL):
            model_name = match.group(1)
            model_body = match.group(2)

            columns = []
            relationships = []

            # Parse each line in the model body
            for line in model_body.strip().split('\n'):
                line = line.strip()
                if not line or line.startswith('//') or line.startswith('@@'):
                    continue

                # Pattern: fieldName Type[@decorators]
                field_match = re.match(r'(\w+)\s+(\w+)(\[\])?\??(\s+.*)?', line)
                if field_match:
                    field_name = field_match.group(1)
                    field_type = field_match.group(2)
                    is_array = field_match.group(3) is not None
                    decorators = field_match.group(4) or ''

                    # Check if it's a relation (type is another model)
                    if field_type[0].isupper() and field_type not in ['String', 'Int', 'Float', 'Boolean', 'DateTime', 'Json', 'Bytes', 'Decimal', 'BigInt']:
                        relationships.append({
                            'column': field_name,
                            'references': field_type,
                            'type': 'one_to_many' if is_array else 'many_to_one'
                        })
                    else:
                        columns.append({
                            'name': field_name,
                            'type': field_type,
                            'primary_key': '@id' in decorators,
                            'nullable': '?' in line,
                            'foreign_key': None
                        })

            if columns:
                tables.append({
                    'name': model_name,
                    'columns': columns,
                    'primary_key': next((c['name'] for c in columns if c.get('primary_key')), 'id'),
                    'relationships': relationships,
                    'file': file_path,
                    'type': 'prisma'
                })

        return tables

    def _extract_typescript_models(self, content: str, file_path: str) -> list:
        """Extract TypeORM/Sequelize models with columns and relationships."""
        import re
        tables = []

        if '@Entity' not in content and 'sequelize' not in content.lower():
            return tables

        # Find TypeORM entities
        entity_pattern = r'@Entity\s*\([^)]*\)\s*(?:export\s+)?class\s+(\w+)'
        for match in re.finditer(entity_pattern, content):
            entity_name = match.group(1)
            columns = []
            relationships = []

            # Find columns
            col_pattern = r'@(?:PrimaryGeneratedColumn|Column)\s*\([^)]*\)\s*(\w+)\s*[?:]?\s*(\w+)?'
            for col_match in re.finditer(col_pattern, content):
                col_name = col_match.group(1)
                col_type = col_match.group(2) or 'string'
                is_pk = 'PrimaryGeneratedColumn' in col_match.group(0)
                columns.append({
                    'name': col_name,
                    'type': col_type,
                    'primary_key': is_pk,
                    'nullable': True,
                    'foreign_key': None
                })

            # Find relationships
            rel_pattern = r'@(?:ManyToOne|OneToMany|OneToOne|ManyToMany)\s*\([^)]*\)\s*(\w+)\s*[?:]?\s*(\w+)?'
            for rel_match in re.finditer(rel_pattern, content):
                rel_name = rel_match.group(1)
                rel_type = rel_match.group(2)
                if rel_type:
                    relationships.append({
                        'column': rel_name,
                        'references': rel_type.replace('[]', ''),
                        'type': 'one_to_many' if 'OneToMany' in rel_match.group(0) else 'many_to_one'
                    })

            if columns:
                tables.append({
                    'name': entity_name,
                    'columns': columns,
                    'primary_key': next((c['name'] for c in columns if c.get('primary_key')), 'id'),
                    'relationships': relationships,
                    'file': file_path,
                    'type': 'typeorm'
                })

        return tables

    def _extract_java_models(self, content: str, file_path: str) -> list:
        """Extract JPA/Hibernate entities with columns and relationships."""
        import re
        tables = []

        if '@Entity' not in content:
            return tables

        # Find entity classes
        entity_pattern = r'@Entity\s*(?:@Table\s*\([^)]*\))?\s*public\s+class\s+(\w+)'
        for match in re.finditer(entity_pattern, content):
            entity_name = match.group(1)
            columns = []
            relationships = []

            # Find fields
            field_pattern = r'private\s+(\w+)\s+(\w+)\s*;'
            for field_match in re.finditer(field_pattern, content):
                field_type = field_match.group(1)
                field_name = field_match.group(2)

                # Check if it's a relation type
                if field_type[0].isupper() and field_type not in ['String', 'Integer', 'Long', 'Double', 'Float', 'Boolean', 'Date', 'LocalDateTime']:
                    relationships.append({
                        'column': field_name,
                        'references': field_type.replace('List<', '').replace('>', '').replace('Set<', ''),
                        'type': 'one_to_many' if 'List<' in content[field_match.start()-50:field_match.start()] else 'many_to_one'
                    })
                else:
                    columns.append({
                        'name': field_name,
                        'type': field_type,
                        'primary_key': '@Id' in content[field_match.start()-100:field_match.start()],
                        'nullable': True,
                        'foreign_key': None
                    })

            if columns:
                tables.append({
                    'name': entity_name,
                    'columns': columns,
                    'primary_key': next((c['name'] for c in columns if c.get('primary_key')), 'id'),
                    'relationships': relationships,
                    'file': file_path,
                    'type': 'jpa'
                })

        return tables

    def _extract_sql_tables(self, content: str, file_path: str) -> list:
        """Extract tables from SQL CREATE TABLE statements."""
        import re
        tables = []

        # Find CREATE TABLE statements
        table_pattern = r'CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?[`"\']?(\w+)[`"\']?\s*\(([^;]+)\)'
        for match in re.finditer(table_pattern, content, re.IGNORECASE | re.DOTALL):
            table_name = match.group(1)
            table_body = match.group(2)

            columns = []
            relationships = []

            # Parse columns
            lines = table_body.split(',')
            for line in lines:
                line = line.strip()

                # Skip constraints
                if re.match(r'^\s*(PRIMARY|FOREIGN|UNIQUE|INDEX|CONSTRAINT|KEY)', line, re.IGNORECASE):
                    # Check for foreign key
                    fk_match = re.search(r'FOREIGN\s+KEY\s*\([`"\']?(\w+)[`"\']?\)\s*REFERENCES\s+[`"\']?(\w+)[`"\']?', line, re.IGNORECASE)
                    if fk_match:
                        col_name = fk_match.group(1)
                        ref_table = fk_match.group(2)
                        relationships.append({
                            'column': col_name,
                            'references': self._snake_to_pascal(ref_table),
                            'type': 'many_to_one'
                        })
                    continue

                # Parse column definition
                col_match = re.match(r'[`"\']?(\w+)[`"\']?\s+([\w\(\)]+)', line)
                if col_match:
                    col_name = col_match.group(1)
                    col_type = col_match.group(2).upper()
                    columns.append({
                        'name': col_name,
                        'type': col_type,
                        'primary_key': 'PRIMARY KEY' in line.upper(),
                        'nullable': 'NOT NULL' not in line.upper(),
                        'foreign_key': None
                    })

            if columns:
                tables.append({
                    'name': self._snake_to_pascal(table_name),
                    'columns': columns,
                    'primary_key': next((c['name'] for c in columns if c.get('primary_key')), 'id'),
                    'relationships': relationships,
                    'file': file_path,
                    'type': 'sql'
                })

        return tables

    def _snake_to_pascal(self, name: str) -> str:
        """Convert snake_case to PascalCase."""
        return ''.join(word.title() for word in name.split('_'))


# Singleton instance
chunked_document_agent = ChunkedDocumentAgent()
