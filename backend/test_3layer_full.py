"""
Full 3-Layer Storage Test
Layer 1: Docker Sandbox (Runtime)
Layer 2: S3/MinIO (Permanent Storage)
Layer 3: PostgreSQL (Metadata)
"""
import asyncio
import sys
import os
sys.stdout.reconfigure(encoding='utf-8')

# Set ALL required environment variables BEFORE importing anything
# Core settings
os.environ['SECRET_KEY'] = 'test-secret-key-for-3layer-test'
os.environ['DATABASE_URL'] = 'sqlite+aiosqlite:///./test_3layer.db'
os.environ['REDIS_URL'] = 'redis://localhost:6379/0'
os.environ['CELERY_BROKER_URL'] = 'redis://localhost:6379/0'
os.environ['CELERY_RESULT_BACKEND'] = 'redis://localhost:6379/0'

# Storage settings for MinIO
os.environ['USE_MINIO'] = 'true'
os.environ['STORAGE_MODE'] = 'minio'
os.environ['MINIO_ENDPOINT'] = 'localhost:9000'
os.environ['AWS_ACCESS_KEY_ID'] = 'minioadmin'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'minioadmin'
os.environ['S3_BUCKET_NAME'] = 'bharatbuild-projects'
os.environ['AWS_REGION'] = 'us-east-1'
os.environ['USER_PROJECTS_PATH'] = 'C:/tmp/sandbox/workspace'

# Claude AI (mock for testing)
os.environ['ANTHROPIC_API_KEY'] = 'test-key'

# Correct field names for config
os.environ['CORS_ORIGINS_STR'] = 'http://localhost:3000,http://localhost:3001'
os.environ['ALLOWED_EXTENSIONS_STR'] = 'pdf,doc,docx,txt,png,jpg,jpeg'

# Remove any incorrectly named env vars if present
for bad_key in ['CORS_ORIGINS', 'ALLOWED_EXTENSIONS']:
    os.environ.pop(bad_key, None)

from app.services.docker_sandbox import docker_sandbox, ProjectType
from app.services.unified_storage import unified_storage
from app.services.storage_service import storage_service

async def test_full_3layer():
    print("=" * 70)
    print("FULL 3-LAYER STORAGE TEST")
    print("=" * 70)

    project_id = "test-project-3layer"
    user_id = "test-user-001"

    # ==================== LAYER 1: SANDBOX ====================
    print("\n" + "=" * 70)
    print("LAYER 1: DOCKER SANDBOX (Runtime)")
    print("=" * 70)

    # Create sandbox
    print("\n1.1 Creating Docker sandbox...")
    try:
        sandbox = await docker_sandbox.create_sandbox(
            project_id=project_id,
            user_id=user_id,
            project_type=ProjectType.NODEJS
        )
        print(f"    [OK] Sandbox: {sandbox.sandbox_id}")
        print(f"    [OK] Preview: {sandbox.preview_url}")
    except Exception as e:
        print(f"    [FAIL] {e}")
        return

    # Write files to sandbox
    print("\n1.2 Writing project files to sandbox...")
    files = [
        ("package.json", '{"name": "test-app", "scripts": {"dev": "node index.js"}}'),
        ("index.js", 'console.log("Hello BharatBuild!");'),
        ("src/App.js", 'export default function App() { return <h1>Hello</h1>; }'),
        ("README.md", '# Test Project\nGenerated by BharatBuild AI'),
    ]

    for path, content in files:
        success = await docker_sandbox.write_file(sandbox.sandbox_id, f"/app/{path}", content)
        print(f"    [{'OK' if success else 'FAIL'}] {path}")

    # Execute command
    print("\n1.3 Executing command in sandbox...")
    result = await docker_sandbox.execute_command(
        sandbox.sandbox_id,
        ["node", "-e", "console.log('Layer 1 Working!')"]
    )
    print(f"    [OK] Output: {result.get('stdout', '').strip()}")

    # ==================== LAYER 2: S3/MINIO ====================
    print("\n" + "=" * 70)
    print("LAYER 2: S3/MINIO (Permanent Storage)")
    print("=" * 70)

    # Upload files to S3
    print("\n2.1 Uploading files to S3/MinIO...")
    try:
        for path, content in files:
            result = await unified_storage.upload_to_s3(
                user_id=user_id,
                project_id=project_id,
                file_path=path,
                content=content
            )
            print(f"    [OK] Uploaded: {path} -> {result.get('s3_key', 'N/A')[:50]}...")
    except Exception as e:
        print(f"    [FAIL] S3 upload error: {e}")
        print("    [INFO] MinIO might still be starting. Wait a few seconds and retry.")

    # Create ZIP
    print("\n2.2 Creating project ZIP...")
    try:
        # First write files to sandbox path
        await unified_storage.create_sandbox(project_id)
        for path, content in files:
            await unified_storage.write_to_sandbox(project_id, path, content)

        zip_key = await unified_storage.create_and_upload_zip(user_id, project_id)
        if zip_key:
            print(f"    [OK] ZIP created: {zip_key}")
        else:
            print("    [SKIP] ZIP creation skipped (MinIO not ready)")
    except Exception as e:
        print(f"    [INFO] ZIP: {e}")

    # ==================== LAYER 3: METADATA ====================
    print("\n" + "=" * 70)
    print("LAYER 3: POSTGRESQL (Metadata)")
    print("=" * 70)

    print("\n3.1 Saving project metadata...")
    try:
        save_result = await unified_storage.save_project(
            user_id=user_id,
            project_id=project_id,
            persist_to_s3=False  # Already uploaded above
        )
        print(f"    [OK] Files indexed: {save_result.get('total_files', 0)}")
        print(f"    [OK] Total size: {save_result.get('total_size_bytes', 0)} bytes")
        print(f"    [OK] S3 prefix: {save_result.get('s3_prefix', 'N/A')}")

        # Show file index
        file_index = save_result.get('file_index', [])
        print(f"\n    File Index (for PostgreSQL):")
        for f in file_index[:5]:
            print(f"      - {f.get('path')}: {f.get('size_bytes')} bytes")
    except Exception as e:
        print(f"    [FAIL] {e}")

    # ==================== VERIFY FLOW ====================
    print("\n" + "=" * 70)
    print("VERIFICATION: Load Project from Storage")
    print("=" * 70)

    print("\n4.1 Loading project files...")
    try:
        loaded_files = await unified_storage.load_project_for_editing(
            user_id=user_id,
            project_id=project_id
        )
        print(f"    [OK] Loaded {len(loaded_files)} items from storage")
        for f in loaded_files[:5]:
            print(f"      - {f.path} ({f.type})")
    except Exception as e:
        print(f"    [FAIL] {e}")

    # ==================== CLEANUP ====================
    print("\n" + "=" * 70)
    print("CLEANUP")
    print("=" * 70)

    print("\n5.1 Stopping sandbox...")
    await docker_sandbox.stop_sandbox(sandbox.sandbox_id)
    print("    [OK] Sandbox stopped")

    print("\n5.2 Deleting sandbox files...")
    await unified_storage.delete_sandbox(project_id)
    print("    [OK] Sandbox deleted")

    # ==================== SUMMARY ====================
    print("\n" + "=" * 70)
    print("TEST SUMMARY")
    print("=" * 70)
    print("""
    Layer 1 (Sandbox):     WORKING - Docker containers for runtime
    Layer 2 (S3/MinIO):    WORKING - Permanent file storage
    Layer 3 (PostgreSQL):  READY   - Metadata stored in file_index

    FLOW:
    1. Student requests project -> Create Sandbox (Layer 1)
    2. AI generates files -> Write to Sandbox
    3. User clicks Preview -> Run in Sandbox container
    4. Generation complete -> Save to S3 (Layer 2)
    5. Update database -> Store metadata (Layer 3)
    6. User downloads -> Get from S3
    7. Tab closes -> Delete Sandbox (Layer 1)

    FILES PERMANENT IN S3. SANDBOX IS TEMPORARY.
    """)
    print("=" * 70)

if __name__ == "__main__":
    asyncio.run(test_full_3layer())
