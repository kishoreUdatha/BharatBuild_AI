{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# BharatBuild Qwen Fine-tuning (Kaggle)\n\n**GPU:** T4 x2 (Free)\n**Time:** ~3-4 hours\n**Cost:** FREE",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Cell 1: Check GPU\nimport torch\nprint(f\"GPUs available: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 2: Install dependencies\n!pip install -q transformers>=4.37.0 datasets>=2.16.0 accelerate>=0.25.0\n!pip install -q peft>=0.7.0 bitsandbytes>=0.41.0 trl>=0.7.0\n!pip install -q scipy sentencepiece",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 3: Upload training data\n# Click the '+' icon on the right panel -> Upload -> Select train.jsonl\n# Or drag and drop your file to the Input section\n\n# After upload, your file will be at:\n# /kaggle/input/your-dataset-name/train.jsonl\n\n# For now, let's upload directly:\nfrom IPython.display import display, HTML\ndisplay(HTML('<h3>Upload train.jsonl using the panel on the right (+ Add Data)</h3>'))\n\nimport os\n# Check if file exists\nif os.path.exists('/kaggle/input'):\n    print(\"Input folder contents:\")\n    !ls -la /kaggle/input/\nelse:\n    print(\"No input data yet. Upload your train.jsonl file.\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 4: Set the data file path\n# IMPORTANT: Update this path after uploading your file!\n\n# Option A: If you uploaded as a dataset:\n# DATA_FILE = \"/kaggle/input/your-dataset-name/train.jsonl\"\n\n# Option B: Upload directly to working directory:\n# Run this to upload:\nfrom google.colab import files\ntry:\n    uploaded = files.upload()\n    DATA_FILE = \"/kaggle/working/train.jsonl\"\n    !mv train.jsonl /kaggle/working/ 2>/dev/null || true\nexcept:\n    # If not in Colab-compatible mode, set path manually\n    DATA_FILE = \"/kaggle/input/training-data/train.jsonl\"  # Update this!\n    print(f\"Set DATA_FILE to: {DATA_FILE}\")\n    print(\"Update the path above if different!\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 5: Configuration\nimport os\nimport torch\nfrom datetime import datetime\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\n\n# Model\nMODEL_NAME = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\nOUTPUT_DIR = \"/kaggle/working/qwen-bharatbuild\"\n\n# Training params - optimized for Kaggle T4\nBATCH_SIZE = 2\nGRADIENT_ACCUMULATION = 8\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 3\nMAX_SEQ_LENGTH = 1024  # Reduced for speed\n\n# LoRA\nLORA_R = 32  # Reduced for faster training\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.05\n\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 6: Load tokenizer and model\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, padding_side=\"right\")\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"Configuring 4-bit quantization...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(f\"Loading model: {MODEL_NAME}\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nprint(\"Model loaded!\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 7: Apply LoRA\nprint(\"Applying LoRA...\")\nlora_config = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 8: Load and format dataset\ndef format_training_sample(example):\n    messages = example['messages']\n    system_msg = \"\"\n    user_msg = \"\"\n    assistant_msg = \"\"\n    for msg in messages:\n        if msg['role'] == 'system':\n            system_msg = msg['content']\n        elif msg['role'] == 'user':\n            user_msg = msg['content']\n        elif msg['role'] == 'assistant':\n            assistant_msg = msg['content']\n    return {\"text\": \"<|im_start|>system\\n\" + system_msg + \"<|im_end|>\\n<|im_start|>user\\n\" + user_msg + \"<|im_end|>\\n<|im_start|>assistant\\n\" + assistant_msg + \"<|im_end|>\"}\n\nprint(f\"Loading dataset from {DATA_FILE}...\")\ndataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\nprint(f\"Dataset size: {len(dataset)} samples\")\ndataset = dataset.map(format_training_sample, remove_columns=dataset.column_names)\nprint(\"Dataset ready!\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 9: Setup trainer\nconfig = SFTConfig(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n    learning_rate=LEARNING_RATE,\n    packing=False,  # Disabled for stability\n    fp16=True,  # Use fp16 instead of bf16 for T4\n    gradient_checkpointing=True,\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=2,\n    warmup_steps=50,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=config,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n)\nprint(\"Trainer ready!\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 10: START TRAINING\nprint(\"=\" * 60)\nprint(\"Starting training...\")\nprint(f\"Start time: {datetime.now()}\")\nprint(\"=\" * 60)\n\ntrainer.train()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Training complete!\")\nprint(f\"End time: {datetime.now()}\")\nprint(\"=\" * 60)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 11: Save model\nprint(\"Saving model...\")\ntrainer.save_model(f\"{OUTPUT_DIR}/final\")\ntokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\nprint(f\"Model saved to: {OUTPUT_DIR}/final\")\n\n# List saved files\n!ls -la {OUTPUT_DIR}/final/",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 12: Download model\n# Zip and download\n!cd /kaggle/working && zip -r qwen-bharatbuild-finetuned.zip qwen-bharatbuild/final\n\nprint(\"\\nModel zipped! Download from the Output section on the right panel.\")\nprint(\"Or click: Output -> qwen-bharatbuild-finetuned.zip -> Download\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
