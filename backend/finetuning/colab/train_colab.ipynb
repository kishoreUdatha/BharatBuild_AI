{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BharatBuild Qwen Fine-tuning (Google Colab)\n",
        "\n",
        "This notebook fine-tunes Qwen2.5-Coder-7B for BharatBuild AI.\n",
        "\n",
        "**Requirements:**\n",
        "- Colab Pro ($10/month) for T4 GPU\n",
        "- Or Colab Pro+ for A100 GPU (faster)\n",
        "\n",
        "**Estimated Time:** 4-6 hours on T4"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install dependencies\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q transformers>=4.37.0 datasets>=2.16.0 accelerate>=0.25.0\n",
        "!pip install -q peft>=0.7.0 bitsandbytes>=0.41.0 trl>=0.7.0\n",
        "!pip install -q scipy sentencepiece"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Upload training data\n",
        "from google.colab import files\n",
        "print(\"Upload your training_data.jsonl file:\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Move uploaded file\n",
        "!mkdir -p data\n",
        "!mv training_data.jsonl data/ 2>/dev/null || mv train.jsonl data/training_data.jsonl 2>/dev/null || echo \"File already in place\"\n",
        "!wc -l data/training_data.jsonl"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training Configuration\n",
        "import os\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "OUTPUT_DIR = \"./output/qwen-bharatbuild\"\n",
        "DATA_FILE = \"./data/training_data.jsonl\"\n",
        "\n",
        "# Optimized for T4 (16GB) - reduce batch size if OOM\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION = 16  # Effective batch size = 16\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 3\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 128\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Load Model and Tokenizer\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4-bit quantization config\n",
        "print(\"Configuring 4-bit quantization...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Apply LoRA\n",
        "print(\"Applying LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Load and Format Dataset\n",
        "def format_training_sample(example):\n",
        "    return {\n",
        "        \"text\": f\"<|im_start|>system\\n{example.get('system', 'You are a helpful coding assistant.')}<|im_end|>\\n\"\n",
        "                f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\\n\"\n",
        "                f\"<|im_start|>assistant\\n{example['output']}<|im_end|>\"\n",
        "    }\n",
        "\n",
        "print(f\"Loading dataset from {DATA_FILE}...\")\n",
        "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
        "print(f\"Dataset size: {len(dataset)} samples\")\n",
        "dataset = dataset.map(format_training_sample, remove_columns=dataset.column_names)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Setup Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=0.3,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    packing=True,\n",
        ")\n",
        "print(\"Trainer ready!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: START TRAINING\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting training...\")\n",
        "print(f\"Start time: {datetime.now()}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training complete!\")\n",
        "print(f\"End time: {datetime.now()}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Save Model\n",
        "print(\"Saving model...\")\n",
        "trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
        "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\n",
        "print(f\"Model saved to: {OUTPUT_DIR}/final\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Download Model\n",
        "!zip -r qwen-bharatbuild-finetuned.zip {OUTPUT_DIR}/final\n",
        "\n",
        "from google.colab import files\n",
        "files.download('qwen-bharatbuild-finetuned.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Upload to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r {OUTPUT_DIR}/final /content/drive/MyDrive/qwen-bharatbuild-finetuned/"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
